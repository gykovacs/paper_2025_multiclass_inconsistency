% Template for Elsevier CRC journal article
% version 1.1 dated 16 March 2010

% This file (c) 2010 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Procedia Computer Science
% but may easily be adapted to other journals

% Changes since version 1.0
% - elsarticle class option changed from 1p to 3p (to better reflect CRC layout)

%-----------------------------------------------------------------------------------

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at https://www.elsevier.com/latex

%-----------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Important note on usage                  %%
%% -----------------------                  %%
%% This file must be compiled with PDFLaTeX %%
%% Using standard LaTeX will not work!      %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The '3p' and 'times' class options of elsarticle are used for Elsevier CRC
\documentclass[3p,times]{elsarticle}

%% The `ecrc' package must be called to make the CRC functionality available
\usepackage{ecrc}
\usepackage{amsmath}
\usepackage{lscape}
%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00'
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
\journalname{Applied Soft Computing}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{A. Fazekas and G. Kov\'acs}

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g. if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{Appl. Soft Comput.}

%% Give a short journal name for the dummy logo (if needed)
\jnltitlelogo{Applied Soft Computing}

%% Hereafter the template follows `elsarticle'.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
%\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

%\dochead{}
%% Use \dochead if there is an article header, e.g. \dochead{Short communication}

%\title{Numerical Technique For Testing the Coherence of Reported Performance Metrics Of Binary Segmentation Methods}
\title{Testing the Consistency of Performance Scores Reported for Binary Classification Problems}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[1]{Attila Fazekas} 
\ead{fazekas.attila@inf.unideb.hu}

\author[2]{Gy\"orgy Kov\'acs}
\ead{gyuriofkovacs@gmail.com}


\affiliation[1]{organization={Faculty of Informatics, University of Debrecen},
    addressline={Kassai út 26}, 
    city={Debrecen},
    postcode={4028}, 
    country={Hungary}}
\affiliation[2]{organization={Analytical Minds},
    addressline={Árpád út 5}, 
    city={Beregsurány},
    postcode={4933}, 
    country={Hungary}}

\begin{abstract}
Binary classification is one of the most basic tasks of machine learning, driving countless applications in various fields of science. Regardless of fundamental research being done or applications being improved, classification techniques are evaluated, compared and ranked in terms of performance metrics like \emph{accuracy}, \emph{sensitivity}, \emph{specificity}. In many cases, due to undisclosed and differing cross-validation schemes, aggregation schemes, typos, etc., the reported performance scores do not provide a reliable basis for the ranking of approaches (). Since the various performance scores describe the same evaluation results, they cannot take any values independently, they need to maintain some internal consistency. In this paper, we propose numerical techniques to test the internal consistency of reported performance scores in various evaluation scenarios. The tests we developed are not statistical but numerical: inconsistencies are identified with certainty. In various applications we show how the proposed technique can be used to identify unorthodox evaluation methodologies and prevent the derailing of entire fields. For the benefit of the community, we have released the tests in an open-source Python package.
%Binary classifiers and their applications are crucial in many fields, and measuring the success of binary classification typically involves using performance metrics such as accuracy, sensitivity, and specificity. This paper introduces a mathematical toolkit to ensure the coherence of published performance metrics and detect any miscalculations or errors. Spotting errors in performance scores is important for the scientific community, as they can significantly impact algorithm rankings. Unfortunately, our experiments have shown that this problem is not insignificant. As a solution, we have developed a freely available Python software package to assist with this issue. We aspire to make a positive contribution towards minimizing errors that may arise during the publishing the results of scientific experiments.
\end{abstract}

\begin{keyword}
binary classification
\sep 
performance metrics
\sep
ranking of binary classifiers
\sep 
coherence of performance metrics
\end{keyword}


\end{frontmatter}

        
% Research highlights
%\begin{highlights}
%\item Valami1.
%\item Valami2.
%\item Valami3.
%\item Valami4.
%\item Valami5.
%\end{highlights}

% Keywords
% Each keyword is seperated by \sep
% https://www.elsevier.com/journals/computers-and-education/0360-1315/guide-for-authors#txt42001

%%% Introduction %%%
\section{Introduction}\label{section:Introduction}

In binary classification tasks, the goal is to accurately assign elements from a given set into one of two predetermined classes. While this may seem straightforward in theory, it can be challenging in practice. Defining clear classification rules and obtaining accurate data for evaluation can be difficult, leading to errors in classification. As a result, achieving perfect binary classification can be a complex task.\cite{1}

To address the challenges of binary classification, various methods have been developed to compensate for the lack of precise knowledge about classification rules or imprecise data. These methods aim to provide the best possible solution given the specifics of the problem at hand. Some of the most commonly used methods for binary classification include decision trees, random forests, Bayesian networks, support vector machines, and neural networks.\cite{2}

The significance of binary classification is further highlighted by its applicability to multiclass classification problems, where instances must be classified into one of three or more classes. This can be achieved by reducing the multiclass problem into multiple binary classification problems using approaches such as one-vs-rest and one-vs-one.\cite{3}

The effectiveness of binary classification methods is typically evaluated using performance metrics that measure the number of correctly and incorrectly classified instances for each class. Common performance metrics include accuracy, sensitivity, and specificity, among others.\cite{4}

Binary classification algorithms have a wide range of applications across various fields. For example, in digital image processing, binary classification can be used for tasks such as cell segmentation and vessel network segmentation \cite{5,6}, or exudatum segmentation.

Binary classification is an active area of research, with numerous scientific publications released each year that evaluate the effectiveness of new methods using performance metrics. These metrics play a crucial role in the acceptance of new research by the scientific community, as new methods must demonstrate comparable or superior performance to previously published methods. This has led to a ‘war of numbers’ in the field, where researchers strive to achieve the best possible performance scores. \cite{7}

As a result of the emphasis on performance metrics in binary classification research, it is crucial to ensure the reliability of these metrics. Any inaccuracies in their calculation can impact the performance ranking of different algorithms, making it essential to accurately determine and evaluate these scores.

To accurately compare the performance of different binary classification algorithms, it is essential to calculate their performance metrics using the same principles and benchmarks. These benchmarks must have well-defined and clear steps to ensure consistency and reliability in performance evaluation.

This paper highlights the importance of using clear and easy-to-follow benchmarks when comparing the performance of binary classification algorithms. Additionally, it provides a mathematical toolkit for verifying the consistency of performance metrics, allowing for the early detection of potential errors. This method can also be used to evaluate previously published results, helping to prevent misunderstandings and mistakes from impacting the evaluation of new algorithms.

This paper is structured as follows: Chapter 2 introduces the problem of binary classification and describes commonly used performance metrics, their calculation, and interpretation. It also outlines the task of determining consistency among these metrics. Chapter 3 presents a mathematical method for verifying the consistency of performance metrics for a given data set in a binary classification task. Chapter 4 demonstrates how the results from Chapter 3 can be applied aggregated performance scores of data sets. Chapter 5 provides some example of how our method can be used to compare results from the literature. Finally, Chapter 6 discusses our future research ideas and directions.

%%% Binary classification and its performance %%%
\section{Binary classification and its performance}\label{section:BinaryClassification}

A binary classification task can be formally stated as: Let $X$ be a finite set of elements (data) $x_1,x_2,\dots,x_n$. Let $X_0$ and $X_1$ ($X_0\cup X_1=X$ and $X_0\cap X_1=\emptyset$) two classes. Our task is to determine the binary classifier $f$ that satisfies the following condition for all $x_i\in X$: 
\begin{equation}
f(x_i) = \begin{cases}
  0 &\hbox{if } x_i\in X_0, \\
  1 &\hbox{if } x_i\in X_1.
\end{cases}    
\end{equation}
Let $X_1$ be called the class of examples or positive examples and $X_0$ the class of counterexamples or negative examples. 
Given a classifier $f$ on a specific data set $X$, there are four basic combinations of actual data label and assigned label: 
\begin{itemize}
    \item True Positives (TP):  $X_{TP}=\{x_i\vert x_i\in X_1\hbox{ and }f(x_i)=1\}$
    \item True Negatives (TN):  $X_{TN}=\{x_i\vert x_i\in X_0\hbox{ and }f(x_i)=0\}$
    \item False Positives (FP): $X_{FP}=\{x_i\vert x_i\in X_0\hbox{ and }f(x_i)=1\}$
    \item False Negatives (FN): $X_{FN}=\{x_i\vert x_i\in X_1\hbox{ and }f(x_i)=0\}$
\end{itemize}
 
In the rest of the paper, we introduce the following notations for the number of elements of four above defined data sets: $\hbox{TP}=\vert X_{TP}\vert$, $\hbox{TN}=\vert X_{TN}\vert$, $\hbox{FP}=\vert X_{FP}\vert$, and $\hbox{FN}=\vert X_{FN}\vert$.

In actuality, binary classifiers can at times produce erroneous classifications. To quantify the nature and scope of such misclassifications, various performance metrics are conventionally employed in literature. Table~\ref{table1}, which is mainly based on \cite{9}, provides a summary of the most commonly used performance metrics.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
    \hline
    Notation&Metric&Definition\\
    \hline
     &&\\
     acc&Accuracy&$\frac{TP+TN}{TP+TN+FP+FN}$  \\
     &&\\
     err&Error Rate&$\frac{FP+FN}{TP+TN+FP+FN}$ \\
     &&\\
     ppv&Positive Predactive Value&$\frac{TP}{TP+FP}$\\
     &&\\
     sens&Sensitivity&$\frac{TP}{TP+FN}$\\
     &&\\
     spec&Specificity&$\frac{TN}{TN+FP}$  \\
     &&\\
     npv&Negative Predactive Value&$\frac{TN}{TN+FN}$\\
     &&\\
     ba&Balanced Accuracy&$\frac{sens+spec}{2}$ \\
     &&\\
     gm&Geometric Mean&$(sens\cdot spec)^{\frac{1}{2}}$\\
     &&\\
     fm&Fowlkes-Mallows Index&$(sens\cdot ppv)^{\frac{1}{2}}$\\
     &&\\
     $F_1^+$&$F_1^+$-score&2$\cdot \frac{ppv \cdot sens}{ppv+sens}$\\
     &&\\
     $F_1^-$&$F_1^-$-score&$2\cdot \frac{npv \cdot spec}{npv+spec}$\\
     &&\\
     mk&Markedness&$ppv+npv-1$\\
     &&\\
     bm&Bookmaker Informedness&$spec+sens-1$\\
     &&\\
     upm&Unified Performance Measure&$2\cdot\frac{F_1^+\cdot F_1^-}{F_1^++F_1^-}$\\
     &&\\
     mcc&Matthews Correlation Coefficient&
     $(sens\cdot spec\cdot ppv\cdot npv)^{\frac{1}{2}}-(fnr\cdot fpr\cdot for\cdot fdr)^{\frac{1}{2}}$\\
     &&\\
     kp&Cohen’s Kappa&$\frac{acc}{acc+\frac{(TP+TN)\cdot (FP+FN)}{2\cdot (TP\cdot TN-FP\cdot FN)}}$\\
     &&\\
     fpr&False Positive Rate&$\frac{FP}{FP+TN}$\\
     &&\\
     fnr&False Negative Rate&$\frac{FN}{FN+TP}$\\
     &&\\
     fdr&False Discovery Rate&$\frac{FP}{FP+TP}$\\
     &&\\
     for&False Omission Rate&$\frac{FN}{FN+TN}$\\
     &&\\
     pt&Prevalence Threshold&$\frac{(sen\cdot fpr)^{\frac{1}{2}}-fpr}{sen-fpr}$\\
     &&\\
     $lr+$&Positive Likehood Ratio&$\frac{sens}{fpr}$\\
     &&\\
     $lr-$&Negative Likehood Ratio&$\frac{fnr}{spec}$\\
     &&\\
     dor&diagnostic odds ratio&$\frac{lr+}{lr-}$\\
     &&\\
     ji&Jaccard Index&$\frac{TP}{TP+FN+FP}$\\
     &&\\
     \hline
\end{tabular}
\end{center}
\caption{The commonly used performance metrics and their definition}\label{table1}
\end{table}

The equations used to define the performance metrics in the table above clearly demonstrate that they can be expressed using the four metrics (TP, TN, FP, and FN) mentioned previously. Building upon this, we have developed a numerical method to test the coherence between performance metric values associated with a binary classification task.

The core concept of suggested method involves selecting two performance metrics from a set of known of binary classification for a specific data set. The number of elements belonging to the positive and negative classes are also known. This information is then used to formulate a four-equation system with four unknowns, namely TP, TN, FP, and FN. By solving this equation system, we can determine the values of the unknowns, which can be used to calculate any other performance metrics. The coherence test compares the performance metrics obtained through this method with the published other performance metrics.

The key component of our numerical approach is the expression of performance metrics using other two performance metrics.  For this, it is sufficient to be able to express the TP, TN, FP, FN values using the two selected performance metrics. In the case of the pairs of performance metrics most often used in the literature, we prepared two tables (Table~\ref{table:2} and Table~\ref{table:3}) that give these performance metrics as a function of the four parameters named above. The table which depicts the expression of TP, TN, FP, FN values for every conceivable pair of performance metrics cannot be included in the paper due to its extensive size (300 cases). However, utilizing the developed program package enables one to determine the values without any difficulty. The more comprehensive table, comprising correlations for 17 performance metrics, is attached to the article as supplementary material.

\begin{table}[h!]
    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        &TP&TN\\
        \hline
        spec&&\\
        +&$n*spec$&$n*(1 - spec)$\\
        sens&&\\
        \hline
        spec&&\\
        +&$acc*n + acc*p - n*spec$&$n*spec$\\
        acc&&\\
        \hline
        spec&&\\
        +&$\frac{n*ppv*(spec - 1)}{(ppv - 1)}$&$n*spec$\\
        ppv&&\\
        \hline
        spec&&\\
        +&$\frac{p*(npv*spec + npv - spec)}{npv}$&$p*spec$\\
        npv&&\\
        \hline
        sens&&\\
        +&$p*sens$&$acc*n + acc*p - p*sens$\\
        acc&&\\
        \hline
        sens&&\\
        +&$p*sens$&$n + p*sens -\frac{p*sens}{ppv}$\\
        ppv&&\\
        \hline
        sens&&\\
        +&$p*sens$&$\frac{npv*p*(sens - 1)}{(npv - 1)}$\\
        npv&&\\
        \hline
        acc&&\\
        +&$\frac{ppv*(acc*n + acc*p - n)}{(2*ppv - 1)}$&$\frac{(acc*n*ppv - acc*n + acc*p*ppv - acc*p + n*ppv)}{(2*ppv - 1)}$\\
        ppv&&\\
        \hline
        acc&&\\
        +&$\frac{(acc*n*npv - acc*n + acc*npv*p - acc*p + npv*p)}{(2*npv - 1)}$&$\frac{npv*(acc*n + acc*p - p)}{(2*npv - 1)}$\\
        npv&&\\
        \hline
        ppv&&\\
        +&$\frac{ppv*(n*npv - n + npv*p)}{(npv + ppv - 1)}$&$\frac{npv*(n*ppv + p*ppv - p)}{(npv + ppv - 1)}$\\
        npv&&\\
        \hline
    \end{tabular}
    \caption{Connection between the most used performance metrics and the values TP, TN.}\label{table:2}
    \end{center}
\end{table}

\begin{table}
   \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        &FP&FN\\
        \hline
        spec&&\\
        +&$n*(1 - spec)$&$p*(1 - sens)$\\
        sens&&\\
        \hline
        spec&&\\
        +&$n*(1 - spec)$&$-acc*n - acc*p + n*spec + p$\\
        acc&&\\
        \hline
        spec&&\\
        +&$n*(1 - spec)$&$\frac{(-n*ppv*spec + n*ppv + p*ppv - p)}{(ppv - 1)}$\\
        ppv&&\\
        \hline
        spec&&\\
        +&$n - p*spec$&$\frac{p*spec*(1 - npv)}{npv}$\\
        npv&&\\
        \hline
        sens&&\\
        +&$-acc*n - acc*p + n + p*sens$&$p*(1 - sens)$\\
        acc&&\\
        \hline
        sens&&\\
        +&$\frac{p*sens*(1 - ppv)}{ppv}$&$p*(1 - sens)$\\
        ppv&&\\
        \hline
        sens&&\\
        +&$\frac{(n*npv - n - npv*p*sens + npv*p)}{(npv - 1)}$&$p*(1 - sens)$\\
        npv&&\\
        \hline
        acc&&\\
        +&$\frac{(-acc*n*ppv + acc*n - acc*p*ppv + acc*p + n*ppv - n)}{(2*ppv - 1)}$&$\frac{(-acc*n*ppv - acc*p*ppv + n*ppv + 2*p*ppv - p)}{(2*ppv - 1)}$\\
        ppv&&\\
        \hline
        acc&&\\
        +&$\frac{(-acc*n*npv - acc*npv*p + 2*n*npv - n + npv*p)}{(2*npv - 1)}$&$\frac{(-acc*n*npv + acc*n - acc*npv*p + acc*p + npv*p - p)}{(2*npv - 1)}$\\
        npv&&\\
        \hline
        ppv&&\\
        +&$\frac{(-n*npv*ppv + n*npv + n*ppv - n - npv*p*ppv + npv*p)}{(npv + ppv - 1)}$&$\frac{(-n*npv*ppv + n*ppv - npv*p*ppv + npv*p + p*ppv - p)}{(npv + ppv - 1)}$\\
        npv&&\\
        \hline
    \end{tabular}
    \caption{Connection between the most used performance metrics and the values FP, FN.}\label{table:3}
    \end{center}
\end{table}

\begin{landscape}
\begin{table}
\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    &spec&sens&acc&ppv&npv\\
    \hline
    spec&&&&&\\
    +&---&---&$\frac{(n\cdot spec + p\cdot sens)}{(n + p)}$&$\frac{p\cdot sens}{(-n\cdot spec + n + p\cdot sens))}$&$\frac{-n\cdot spec}{(-n\cdot spec + p\cdot sens - p))}$\\
    sens&&&&&\\
    \hline
    spec&&&&&\\
    +&---&$\frac{(acc\cdot n + acc\cdot p - n\cdot spec)}{p}$&---&$\frac{(acc\cdot n + acc\cdot p - n\cdot spec)}{(acc\cdot n + acc\cdot p - 2\cdot n\cdot spec + n))}$&$\frac{-n\cdot spec}{(acc\cdot n + acc\cdot p - 2\cdot n\cdot spec - p))}$\\
    acc&&&&&\\
    \hline
    spec&&&&&\\
    +&---&$\frac{n\cdot ppv\cdot (spec - 1)}{(p\cdot (ppv - 1))}$&$\frac{n\cdot (2\cdot ppv\cdot spec - ppv - spec)}{(n\cdot ppv - n + p\cdot ppv - p}$&---&$\frac{n\cdot spec\cdot (ppv - 1)}{(n\cdot ppv - n\cdot spec + p\cdot ppv - p))}$ \\
    ppv&&&&&\\
    \hline
    spec&&&&&\\
    +&---&$\frac{n\cdot spec}{p} - \frac{n\cdot spec}{(npv\cdot p)}+ 1$&$\frac{(2\cdot n\cdot npv\cdot spec - n\cdot spec + npv\cdot p)}{(npv\cdot (n + p))}$&$\frac{(n\cdot npv\cdot spec - n\cdot spec + npv\cdot p)}{(n\cdot npv - n\cdot spec + npv\cdot p))}$&---\\
    npv&&&&&\\
    \hline
    sens&&&&&\\
    +&$\frac{(acc\cdot n + acc\cdot p - p\cdot sens)}{n}$&---&---&$\frac{-p\cdot sens}{(acc\cdot n + acc\cdot p - n - 2\cdot p\cdot sens))}$&$\frac{(acc\cdot n + acc\cdot p - p\cdot sens)}{(acc\cdot n + acc\cdot p - 2\cdot p\cdot sens + p))}$\\
    acc&&&&&\\
    \hline
    sens&&&&&\\ 
    +&$1 +\frac{p\cdot sens}{n} - \frac{p\cdot sens}{(n\cdot ppv)}$&---&$\frac{(n\cdot ppv + 2\cdot p\cdot ppv\cdot sens - p\cdot sens)}{(ppv\cdot (n + p))}$&---&$\frac{(n\cdot ppv + p\cdot ppv\cdot sens - p\cdot sens)}{(n\cdot ppv + p\cdot ppv - p\cdot sens))}$\\
    ppv&&&&&\\
    \hline
    sens&&&&&\\
    +&$\frac{npv\cdot p\cdot (sens - 1)}{(n\cdot (npv - 1))}$&---&$\frac{p\cdot (2\cdot npv\cdot sens - npv - sens)}{(n\cdot npv - n + npv\cdot p - p)}$&$\frac{p\cdot sens\cdot (npv - 1)}{(n\cdot npv - n + npv\cdot p - p\cdot sens))}$&---\\
    npv&&&&&\\
    \hline
    acc&&&&&\\
    +&$\frac{npv\cdot (acc\cdot n + acc\cdot p - p)}{(n\cdot (2\cdot npv - 1))}$&$\frac{(acc\cdot n\cdot npv - acc\cdot n + acc\cdot npv\cdot p - acc\cdot p + npv\cdot p)}{(p\cdot (2\cdot npv - 1))}$&---&$\frac{-(acc\cdot n\cdot npv - acc\cdot n + acc\cdot npv\cdot p - acc\cdot p + npv\cdot p)}{(acc\cdot n + acc\cdot p - 2\cdot n\cdot npv + n - 2\cdot npv\cdot p))}$&---   \\
    npv&&&&&\\
    \hline
    acc&&&&&\\
    +&$\frac{(acc\cdot n\cdot ppv - acc\cdot n + acc\cdot p\cdot ppv - acc\cdot p + n\cdot ppv)}{(n\cdot (2\cdot ppv - 1))}$&$\frac{ppv\cdot (acc\cdot n + acc\cdot p - n)}{(p\cdot (2\cdot ppv - 1))}$&---&---&$\frac{-(acc\cdot n\cdot ppv - acc\cdot n + acc\cdot p\cdot ppv - acc\cdot p + n\cdot ppv)}{(acc\cdot n + acc\cdot p - 2\cdot n\cdot ppv - 2\cdot p\cdot ppv + p))}$\\
    ppv&&&&&\\
    \hline
    npv&&&&&\\
    +&$\frac{npv\cdot (n\cdot ppv + p\cdot ppv - p)}{(n\cdot (npv + ppv - 1))}$&$\frac{ppv\cdot (n\cdot npv - n + npv\cdot p)}{(p\cdot (npv + ppv - 1))}$&$\frac{(2\cdot n\cdot npv\cdot ppv - n\cdot ppv + 2\cdot npv\cdot p\cdot ppv - npv\cdot p)}{(n\cdot npv + n\cdot ppv - n + npv\cdot p + p\cdot ppv - p)}$&---&---\\
    pvv&&&&&\\
    \hline   
\end{tabular}  
\caption{Connection among the mot used performance metrics}\label{table:4}
\end{table}
\end{landscape}

%%% Coharence of the performance metrics
\section{Coharence of the performance metrics}\label{section:Coharence}

In most cases, we encounter the reality that performance metrics, which describe the performance of a binary classifier, are only available with a fixed precision rounded to a specific decimal point. This renders it impossible to verify the consistency among performance metrics using the straightforward algebraic method presented in the previous chapter.

To address this issue, we introduce a numerical approach in this chapter that enables us to examine coherence while accounting for a possible error margin of $\epsilon$ in the given performance metrics. Instead of relying on the conventional algebraic method, we utilize interval arithmetic to determine the range within which a third performance metric value should fall, considering two given performance metrics with rounded values, in order to establish that the values are consistent.

To illustrate the coherence test, we will provide an example. Three metrics that are commonly used in binary classification literature to assess performance are sensitivity, specificity, and accuracy. For this demonstration, we will focus on sensitivity and specificity and analyze their relationship with accuracy. Our goal is to determine if the published accuracy aligns with the previous two performance metrics.

So, consider a data set with $n$ elements on which we apply a binary classifier with known accuracy, sensitivity, and specificity performance metrics. These values can be understood as functions of the TP, TN, FP, and FN values. If the triplet of $(accuracy, sensitivity, specificity)$ is coherent, we can easily derive the corresponding TP, TN, FP, and FN values by solving the following system of equations:

\begin{align}
sensitivity - \epsilon &\leq \dfrac{TP}{P} \leq sensitivity + \epsilon,\label{eq0}\\
specificity - \epsilon &\leq \dfrac{TN}{N} \leq specificity + \epsilon,\label{eq1}\\
accuracy - \epsilon &\leq \dfrac{TP+TN}{P+N} \leq accuracy + \epsilon,\label{eq2}\\
0 &\leq TP\leq P, \label{eq3}\\
0 &\leq TN\leq N, \label{eq4}
\end{align}

\noindent
with $\epsilon$ indicating the numerical uncertainty of the reported performance scores. The setting $\epsilon = 10^{-k}/2$ is the maximum numerical uncertainty when the performance scores reported to $k$ decimal places are expected to be rounded, and $\epsilon = 10^{-k}$ is the maximum uncertainty when truncation or ceiling to $k$ decimal places is supposed.

Multiplying the first three conditions by $P$, $N$ and $(P+N)$, respectively, and eliminating the $TP$ and $TN$ variables from equations (\ref{eq0}) - (\ref{eq2}) by subtracting (\ref{eq3}) and (\ref{eq4}), one gets 6 conditions which need to hold, independently from the actual values of $TP$, $TN$, $FP$, and $FN$:
\begin{align}
0 &\geq (TN+FN)(acc-spec)+(TP+FP)(acc-sens)- 2\epsilon n, \nonumber\\
0 &\leq (TN+FN)(acc-spec)+(TP+FP)(acc-sens)+ 2\epsilon n, \nonumber\\
0 &\geq (TP+FP)(sens - \epsilon - 1),\nonumber\\
0 &\leq (TP+FP)(sens + \epsilon),\nonumber\\
0 &\geq (TN+FN)(spec - \epsilon - 1),\nonumber\\
0 &\leq (TN+FN)(spec + \epsilon). \label{il-cond}
\end{align}
The conditions (\ref{il-cond}) enable the testing of the consistency of a particular triplet $(acc, sens, spec)$ of the given performance metrics, extracted from a data set with $\epsilon$ numerical uncertainty in $acc$, $sens$, and $spec$. 

When evaluating performance metrics reported in scientific publications, it is important to consider whether the metrics were calculated for the entire data set or for a subset of the data set. If the results were determined for a subset $X^\prime$ (with $\vert X^\prime\vert=n^\prime$) of the set $X$, then the total number of elements $n$ should be replaced with $n^\prime$ in any calculations.

Through the example above, you can easily follow the course of the method, which is implemented in our freely available program package: As described in the previous chapter, we select two of the three available performance metrics. Using the equations that define them -- containing only the parameters $TP$, $TN$, $FP$, and $FN$ -- and equations $P=TP+FP$, $N=TN+FN$, we can calculate the current solution of $TP$, $TN$, $FP$, and $FN$ with interval arithmetic. Here we would like to emphasize that these solutions will actually now be intervals that define the set of possible values for the given $\epsilon$. Using the obtained intervals for $TP$, $TN$, $FP$, and $FN$, we can calculate the possible values of the third, so far unused, performance metric. The reported value of this performance metric must be compared with the result of our calculation. Coherency means that the reported value must fall within the interval we have calculated.

%%% Coherence of the performance metrics determined on k-folds of the data set %%%
\section{Coherence of the aggregated performance metrics determined on $k$ data sets}

Suppose we have a collection of data sets $\{X_i \hskip 1pt\vert\hskip 1pt i=1,\dots,k\in\mathbb{N}\}$. Consider the performance metrics for each data set, denoted by $acc_i$, $sens_i$, and $spec_i$.

Given the exact counts of positive and negative instances for each data set in the collection, we can formulate conditions that must hold if the evaluation was performed according to our assumptions. Specifically, let $P_i$ and $N_i$ denote the counts of positive and negative instances in the manual annotation of the $i$th data set under some assumption. If this assumption is true, then there must exist unknown integers $TP_i$ and $TN_i$ ($i=1, \dots, k$) such that certain conditions are satisfied:

\begin{align}
\dfrac{1}{k}\sum\limits_{i=1}^k{sens_i} - \epsilon \leq \dfrac{1}{k}\sum\limits_{i=1}^{k}\dfrac{TP_i}{P_i} \leq \dfrac{1}{k}\sum\limits_{i=1}^k{sens_i} + \epsilon, \label{cond-sens}\\
\dfrac{1}{k}\sum\limits_{i=1}^k{spec}_i - \epsilon \leq \dfrac{1}{k}\sum\limits_{i=1}^{k}\dfrac{TN_i}{N_i} \leq \dfrac{1}{k}\sum\limits_{i=1}^k{spec}_i + \epsilon, \label{cond-spec}\\
\dfrac{1}{k}\sum\limits_{i=1}^k{acc}_i - \epsilon \leq \dfrac{1}{k}\sum\limits_{i=1}^{k}\dfrac{TP_i + TN_i}{P_i + N_i} \leq \dfrac{1}{k}\sum\limits_{i=1}^k{acc}_i + \epsilon, \label{cond-acc}\\
0 \leq TP_i \leq P_i, i=1, \dots, k, \label{boundary-tp}\\
0 \leq TN_i \leq N_i, i=1, \dots, k. \label{boundary-tn}
\end{align}

Due to the $2*k$ free variables $TP_i$ and $TN_i$, this set of conditions is less straightforward to evaluate, however, one can recognize that all the conditions are linear, thus, the entire set of conditions can be interpreted as the condition set of a linear integer programming problem. In order to illustrate this by representing the problem in the standard form of linear programming, let $\mathbf{x}\in\mathbb{R}^{2*k}$ denote the vector of free variables, with $\mathbf{x}_i$, $i=1,\dots,k$ standing for $TP_i$ and $\mathbf{x}_i, i=k+1\dots,2*k$ representing $TN_i$. Then, introducing the matrix $A\in\mathbb{R}^{6\times 2*k}$ and the vector $\mathbf{b}\in\mathbb{R}^{6}$:
\begin{equation}
A= \dfrac{1}{m}\left[\begin{array}{cccccc}
\frac{1}{P_1} & \dots & \frac{1}{P_{2*k}} & 0 & \dots & 0 \\
-\frac{1}{P_1} & \dots & -\frac{1}{P_{2*k}} & 0 & \dots & 0 \\
0 & \dots & 0 & \frac{1}{N_1} & \dots & \frac{1}{N_{2*k}}\\
0 & \dots & 0 & -\frac{1}{N_1} & \dots & -\frac{1}{N_{2*k}}\\
\frac{1}{P_1 + N_1} & \dots & \frac{1}{P_{2*k} + N_{2*k}} & \frac{1}{P_1+ N_1} & \dots & \frac{1}{P_{2*k}+N_{2*k}} \\
-\frac{1}{P_1 + N_1} & \dots & -\frac{1}{P_{2*k} + N_{2*k}} & -\frac{1}{P_1 + N_1} & \dots & -\frac{1}{P_{2*k}+ N_{2*k}}
\end{array}\right],
\end{equation}
\begin{equation}
b= \left[\begin{array}{c}\dfrac{1}{k}\sum\limits_{i=1}^k{sens_i} + \epsilon \\
-(\dfrac{1}{k}\sum\limits_{i=1}^k{sens_i} - \epsilon) \\
\dfrac{1}{k}\sum\limits_{i=1}^k{spec_i} + \epsilon \\
-(\dfrac{1}{k}\sum\limits_{i=1}^k{spec_i} - \epsilon) \\
\dfrac{1}{k}\sum\limits_{i=1}^k{acc_i} + \epsilon \\
-(\dfrac{1}{k}\sum\limits_{i=1}^k{acc_i} - \epsilon)
\end{array}\right],
\end{equation}
the conditions (\ref{cond-sens})-(\ref{cond-acc}) can be rewritten in the standard form of linear programming
$A\mathbf{x}  \leq \mathbf{b}$,
with the boundary conditions (\ref{boundary-tp}), (\ref{boundary-tn}) and integer constraints on the free variables $\mathbf{x}$. In order to check if these conditions can be fulfilled, one can exploit any linear integer programming solver with a dummy objective function, and if the feasibility set is empty, the solver returns that the conditions cannot be satisfied, indicating that the aggregated performance scores are not consistent under the hypothesis which led to the $P_i$ and $N_i$ counts (using all elements of the data sets or a subsets of them).

%%% Overview of the first experiments %%%
\section{Overview of experimental results}
In this chapter, by providing three specific examples, we aim to demonstrate that published papers in this topics often contains non-negligible errors in its results. We hope that authors will take note of these examples and check their own work thoroughly prior to publication.

\subsection{Retinal vessel segmentation}
Binary classification is a common technique used in digital image processing to accurately detect objects. One example of its application is in retinal vessel segmentation, where the goal is to identify the vascular network on a retinal image \cite{8}. This information can be useful in diagnosing certain eye diseases.

To achieve this, a binary classifier is created to determine whether each pixel in the retinal image belongs to the vascular network. The classifiers are compared using performance metrics such as accuracy, sensitivity, and specificity, sensitivity. The DRIVE database, which contains 20 annotated images of the same size taken with the same camera, is used for this benchmark.

Annotation involves an expert manually selecting the pixels that belong to the vascular network. In the case of DRIVE, independent annotation by two experts is available, with the first expert’s annotation typically used. From this annotation, we can calculate the number of times the binary classifier accurately determined whether a pixel belonged to the vascular network or not. We can also calculate the number of false classifications for both possible errors. These values (TP, TN, FN and FP) are used to calculate the performance metrics. These performance metrics are reported at either the image level or at the level of the image database in various publications.

Ranking lists were compiled based on the performance metrics of various algorithms, with the highest-ranked algorithm being the one with the best performance metrics. The ranking of these algorithms played a significant role in determining their publishability. As such, it was important for us to examine this area as an example of how common errors can occur when publishing performance metrics.

In our previous paper \cite{7}, we analyzed 100 papers on retinal vessel segmentation testing the consistency of published performance metrics. Our study found that nearly 1/3 of the published algorithms had non-coherent performance metrics, resulting in 91\% of the rankings being incorrect. These results demonstrate the usefulness and applicability of this mathematical tool.

\subsection{Exudatum detection}
Another area of computerized examination of eye diseases, in addition to the examination of the vascular network, is the so-called examination of exudatum. Ide kerül majd leírásra a tapasztalat.


\subsection{Ide jön a harmadik, amit mondtál}


%%% Conclusion, further work %%%
\section{Conclusion}
Numerical methods have been described in the paper, which make it possible to check whether these data are coherent or incorrect when specifying at least three different performance metrics. The method can also be used in the aggregate case, when we know the average values of these performance metrics not for a specific image, but for a series of images.

A software tool was created in parallel with the development of the theory, which enables the simple and efficient use of the developed method to check the coherency of published performance metrics.

We also summarize the experience of three specific cases that support the presence of the problem: on the one hand, we refer to our previous work \cite{7}, which initiated the development of this method and points out that in an active and important field such as retinal vessel segmentation, there are problems with the reported performance metrics in one third of the cases, on the other hand we carried out a similar -- if not as large -- examination of ten articles related to exudatum detection, where similar problems were revealed, there is also a third examination... 

%%% Appendix %%%
%\section{Appendix}



%% Loading bibliography style file
% \bibliographystyle{model1-num-names}
\bibliographystyle{elsarticle-num}

% Loading bibliography database
\bibliography{references}

\end{document}
