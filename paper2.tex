% Template for Elsevier CRC journal article
% version 1.1 dated 16 March 2010

% This file (c) 2010 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Procedia Computer Science
% but may easily be adapted to other journals

% Changes since version 1.0
% - elsarticle class option changed from 1p to 3p (to better reflect CRC layout)

%-----------------------------------------------------------------------------------

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at https://www.elsevier.com/latex

%-----------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Important note on usage                  %%
%% -----------------------                  %%
%% This file must be compiled with PDFLaTeX %%
%% Using standard LaTeX will not work!      %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The '3p' and 'times' class options of elsarticle are used for Elsevier CRC
%\documentclass[3p,times]{elsarticle}
%\documentclass[5p, final]{elsarticle}
%\documentclass[3p, times]{elsarticle}
\documentclass[3p, times]{elsarticle}

%% The `ecrc' package must be called to make the CRC functionality available
\usepackage{ecrc}
\usepackage{amsmath}
\usepackage{lscape}
\usepackage{hyperref}
\usepackage{multirow}
%\usepackage{booktabs}
%\usepackage{array}
%\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[noend]{algorithm2e}
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{soul}

%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfig}

\DeclareMathOperator*{\argmax}{arg\,max}
%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00'
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
%\journalname{Applied Soft Computing}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{A. Fazekas and G. Kov\'acs}

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g. if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{Appl. Soft Comput.}

%% Give a short journal name for the dummy logo (if needed)
\jnltitlelogo{Applied Soft Computing}

%% Hereafter the template follows `elsarticle'.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
%\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\lIfElse}[3]{\lIf{#1}{#2 \textbf{ else}~#3}}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

%\dochead{}
%% Use \dochead if there is an article header, e.g. \dochead{Short communication}

\title{Testing the Consistency of Performance Scores Reported for Multiclass Classification Problems}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[1]{Gy\"orgy Kov\'acs}
\ead{gyuriofkovacs@gmail.com}

\author[2]{Avi}
\ead{avi@gmail.com}

\author[3]{Attila Fazekas} 
\ead{attila.fazekas@inf.unideb.hu}



\affiliation[1]{organization={Analytical Minds Ltd.},
    addressline={Árpád út 5}, 
    city={Beregsurány},
    postcode={4933}, 
    country={Hungary}}

\affiliation[2]{organization={Avi Ltd.},
    addressline={Avi street 1}, 
    city={Avi city},
    postcode={4933}, 
    country={US}}

\affiliation[3]{organization={Faculty of Informatics, University of Debrecen},
    addressline={Kassai út 26}, 
    city={Debrecen},
    postcode={4028}, 
    country={Hungary}}

\begin{abstract}
Multi-class classification is a cornerstone of modern machine learning, with widespread applications across scientific and engineering domains. Whether exploring theoretical models or developing practical tools, researchers commonly evaluate and compare classification methods using metrics such as overall accuracy, macro-averaged F1 score, and per-class precision and recall. However, these reported performance metrics may not always provide a reliable basis for comparing research findings. Potential sources of unreliability include unreported methodological choices, misinterpretations of multi-class evaluation protocols, or typographical and computational errors.

In any given experimental setup—defined by a specific number of test samples per class—performance metrics are constrained to a finite and interdependent set of values. Leveraging this property, we propose numerical techniques to verify the internal consistency of reported multi-class classification metrics with their implied experimental configuration. These methods do not rely on statistical inference; instead, they employ numerical tools such as interval analysis and integer linear programming to deterministically identify inconsistencies.

We illustrate the utility of our approach through case studies in diverse medical imaging and diagnostic applications. The proposed consistency tests successfully reveal discrepancies in reported metrics, offering a practical tool for safeguarding the integrity of empirical research. Power analyses indicate that the tests achieve at least 71\% power when performance scores are reported with four decimal places. The tests have already identified inconsistencies in over 100 scientific publications. To support reproducible research practices, we have released our consistency checking tools as part of the open-source Python package mlscorecheck.

\end{abstract}

\begin{keyword}
binary classification
\sep 
performance scores
\sep
ranking of binary classifiers
\sep 
interval arithmetic
\sep
consistency
\end{keyword}


\end{frontmatter}
        
% Research highlights
%\begin{highlights}
%\item Valami1.
%\item Valami2.
%\item Valami3.
%\item Valami4.
%\item Valami5.
%\end{highlights}


%%% Introduction %%%
\thispagestyle{empty}

\section{Introduction}\label{section:Introduction}

Concerns over the \emph{reproducibility crisis} in artificial intelligence and machine learning research have been voiced repeatedly in recent years~\cite{leakage, reprcrisis, repr0, repr1}. A substantial fraction of published findings prove difficult to replicate or rest on questionable methodological foundations. The causes are diverse: missing or inaccessible source code~\cite{leakage}, inappropriate statistical methods~\cite{leakage, staterrors}, selective reporting or ``polishing'' of results~\cite{fabrication}, and inadvertent errors in tabulated metrics all contribute to the problem.

To mitigate these issues, several best-practice guidelines have been proposed for the transparent and rigorous reporting of machine learning experiments~\cite{repr0, repr2}. Yet despite growing awareness, adoption has been slow, and the existing literature remains replete with problematic results. In many subfields of machine learning, research advances are driven by informal benchmarking races, where algorithmic contributions are judged primarily by their reported performance on standard datasets. This dynamic reinforces a focus on numerical scores as the principal---or even sole---indicator of scientific merit. However, comparing performance scores is fraught with challenges, even when the evaluation protocol is fully specified~\cite{ranking}. Once published, inflated or implausible results can lend credibility to flawed procedures and, through citation and replication, propagate distortions across entire fields. Publication bias only exacerbates this effect~\cite{publicationbias}.

Traditional approaches to scrutinizing the reliability of published machine learning results typically involve painstaking manual effort. Analysts may review methodological details for signs of misapplied statistical logic or implementation flaws~\cite{psychiatry, csecurity, satellite}, or they may attempt to reproduce results from scratch---an approach that is both time-consuming and unscalable. In light of these limitations, there is a need for tools that enable automated or semi-automated validation of published metrics.

In this paper, we propose numerical techniques for testing the internal consistency of performance scores reported in multi-class classification settings.

Multi-class classification, a core task in supervised learning~\cite{mlbook}, is not immune to the reproducibility challenges observed in binary problems. Evaluation is typically performed by computing predictions over a labeled test set, often using cross-validation or resampling schemes~\cite{cv1}. From these predictions, a confusion matrix of size $C \times C$ is constructed, where $C$ denotes the number of classes. This matrix captures how often each true class was assigned to each predicted class. In published work, however, the full matrix is rarely presented. Instead, scalar metrics derived from it---such as overall accuracy, macro-averaged F1-score, or per-class recall---are reported, frequently averaged across folds or datasets~\cite{scores}.

Despite their prevalence, these metrics are not unconstrained: they are interdependent and bounded by the structure of the test data. For instance, the size of the confusion matrix is fixed by the number of test examples, and the scores themselves are mathematically tied to the counts in the matrix. This raises an important question: 
\emph{Are the reported metrics consistent with any plausible confusion matrix under the described evaluation setup?} 
Put differently, does there exist a confusion matrix---subject to known constraints such as test set size and score definitions---that could give rise to the published performance values? Complicating this question further are effects such as rounding and aggregation across multiple folds. If no such matrix can exist, then the result is necessarily flawed and cannot be reproduced under the assumed conditions.

In earlier work, tools such as \texttt{DConfusion}~\cite{dconfusion} have attempted to infer partial properties of confusion matrices from reported scores, primarily in binary settings. However, these tools offer limited score coverage and do not account for score aggregation or numerical precision effects, making them prone to overestimating inconsistency. Our previous research introduced exact inference techniques in binary contexts, highlighting how consistent scoring claims can be deterministically verified or falsified using constraint-based reasoning~\cite{vesselsegm, vessel}.

Building on these insights, the present paper extends the consistency checking framework to the general multi-class case. Unlike prior approaches, our method handles a broad range of evaluation metrics, accommodates cross-fold averaging, and rigorously incorporates rounding effects and numerical tolerances. In doing so, we enable deterministic inconsistency detection that is both precise and broadly applicable.

A key advantage of our method is that it is numerical rather than statistical. This means that the detection of an inconsistency implies a logical contradiction in the reported data---\emph{type-I errors (false positives) are impossible}. As in hypothesis testing, there may be false negatives depending on the setup, but we show that in real-world cases the power of the method is often high enough to detect subtle inconsistencies with certainty.

We believe these techniques can serve as valuable tools for meta-research, enabling the community to audit published results and identify cases where evaluation procedures or numerical reporting may be flawed. This contributes to improving the transparency and reproducibility of multi-class classification studies in machine learning.

\subsection*{Contributions}

The main contributions of this work are:
\begin{enumerate}
    \item We present a general-purpose consistency test for performance scores computed from multi-class confusion matrices, supporting over 20 commonly used evaluation metrics.
    \item For scenarios involving score aggregation across folds or datasets, we extend our method to handle averaging schemes while accounting for rounding and interval tolerances.
    \item We demonstrate how the method enables reliable meta-analysis by uncovering incompatible results in real-world use cases.
    \item The implementation is made available through the open-source Python package \texttt{mlscorecheck}~\cite{mlscorecheck}, accessible via PyPI and GitHub: \url{https://github.com/FalseNegativeLab/mlscorecheck}.
\end{enumerate}

\subsection*{Paper Structure}

The remainder of this paper is structured as follows. Section~\ref{sec:problem} introduces the notation, problem setup, and performance metrics used in multi-class evaluation. Sections~\ref{sec:ind} and~\ref{sec:agg} present our proposed consistency checking methods for individual and aggregated scores, respectively. Section~\ref{sec:applications} showcases their application in real-world contexts. Finally, Section~\ref{sec:conclusions} concludes the paper.


\section{Problem formulation}
\label{sec:problem}

In this section, we formulate the problem we address and introduce the notations and concepts that we will reference throughout the rest of the paper.

In binary classification, typically there is a dataset $\mathcal{D} = \lbrace(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_N, y_N)\rbrace$ consisting of $N$ paired \emph{feature vector}s ($\mathbf{x}_i\in\mathbb{R}^{d}$) and \emph{class label}s ($y_i\in\lbrace 0, 1\rbrace$). The classes labeled as $0$ and $1$ are commonly referred to as \emph{negative} and \emph{positive} classes, respectively.
The primary objective of binary classification is to use this dataset to infer (train) a function $h$ capable of making predictions about the class label of a previously unseen feature vector $\mathbf{x}\in\mathbb{R}^d$ as $h(\mathbf{x})$. All classification techniques predict class labels, but many of them (such as decision trees \cite{mlbook}, neural networks \cite{mlbook}, etc.) are designed to estimate class-membership probabilities $\mathbb{P}(\mathbf{x}|y=c)$, $c\in\lbrace 0, 1\rbrace$, and derive class labels by assigning the label with the highest probability, thereby reducing the probability of misclassification \cite{bayesclassifier}. The choice of which classifier outcome to favor depends on the specific application. For instance, image segmentation \cite{segmentation} requires hard labels indicating whether a pixel belongs to an object, whereas many medical applications prioritize ranking cases by the probability (risk) of having a condition \cite{binclasranking}. Consequently, performance measurement varies by the field of application, with two predominant approaches: one quantifies how effectively the class-membership probabilities rank items, typically using the ROC-AUC score \cite{aucsurvey, auc}; the other assesses the quality of label assignment \cite{scores}. This paper focuses on evaluating the quality of labeling.

To eliminate bias, classifiers must be evaluated using feature vectors and corresponding class labels that were not used for training. In the rest of the paper, we refer to these as the \emph{evaluation set}, denoted by $\mathcal{E}$. (We note that in many scenarios the terms \emph{test set} and \emph{validation set} are used synonymously.) For evaluation, the class label $\hat{y} \doteq h(\mathbf{x})$ is predicted for each $(\mathbf{x}, y)\in\mathcal{E}$ and by comparing the corresponding pairs $y$ and $\hat{y}$, the \emph{confusion matrix} (see Table \ref{tptnfpfn}) is constructed, tabulating the integer counts of true positive ($tp$), true negative ($tn$), false positive ($fp$), and false negative ($fn$) predictions. 
One can readily see that if the number of positive $p$ and negative $n$ instances of the evaluation set $\mathcal{E}$ is known, the binary confusion matrix has two degrees of freedom, since $p = tp + fn$ and $n = tn + fp$. Without loss of generality, we consider $tp$ and $tn$ as the independent components.

To facilitate the comparison of classification approaches, a confusion matrix is often summarized by scalar \emph{performance score}s. (We use the term performance score to prevent confusion with the mathematical notions of \emph{measure} and \emph{metric}, which are used synonymously in various sources.) The literature has proposed numerous performance scores, each emphasizing different aspects of performance, some widely used (such as \emph{accuracy}), while others tailored to specific fields (such as the \emph{diagnostic odds ratio} in medical applications \cite{dor}). See Table \ref{tab:scores} for a summary of the scores covered in this paper. In the rest of the paper, we always assume that there is a set $\mathcal{S}\subseteq\lbrace acc, sens, spec, \dots \rbrace$ of scores reported (any subset of the abbreviations in Table \ref{tab:scores}). For a specific score $s\in\mathcal{S}$, the standardized functional form of the score is denoted by $f_s$, with $f_s(tp, tn, p, n)$ yielding the true, possibly infinite decimal value of the score for a confusion matrix characterized by $tp$, $tn$, $p$, and $n$.

%\setlength{\extrarowheight}{2pt}
\begin{table}[t!]
\caption{The potential outcomes in the evaluation of binary classification.}
\label{tptnfpfn}
\begin{small}
\begin{center}
\begin{tabular}{c@{\hspace{4pt}}|@{\hspace{4pt}}c@{\hspace{4pt}}c}
& \multicolumn{2}{c}{predicted} \\
& positive ($\hat{y}_i = 1$) & negative ($\hat{y}_i = 0$) \\ 
\hline
positive ($y_i=1)$ & true positive ($tp$) & false negative ($fn$) \\
negative ($y_i=0)$ & false positive ($fp$) & true negative ($tn$) \\
\end{tabular}
\end{center}
\end{small}
\end{table}

In cases where predefined evaluation sets are not available, the common practice is adopting the \emph{hold-out} approach: randomly partitioning the dataset into two disjoint sets (the training set $\mathcal{T}$ and the evaluation set $\mathcal{E}$).
The random split makes the estimated performance scores uncertain, which can be mitigated by repeating the random partitioning and evaluation multiple times and aggregating the results. To ensure that all data points are represented equally in the evaluation, usually \emph{k-fold cross-validation} (kFCV) is used \cite{cv1}.
In a kFCV scheme, the dataset $\mathcal{D}$ is randomly divided into $k$ disjoint subsets of equal size, referred to as \emph{folds}. The evaluation process iteratively selects one fold as the evaluation set, trains the classifier on the remaining $k-1$ folds, and evaluates it on the selected fold, using all data points for evaluation once. Finally, the results are aggregated over all folds. 
To improve stability further, kFCV can be repeated multiple times with different random partitionings of $\mathcal{D}$ (known as \emph{repeated kFCV}). Another common enhancement to kFCV is applying \emph{stratification} to ensure that the class distributions in each fold approximate that of the entire dataset. We also note that in some cases, classifiers are evaluated on and the results aggregated over multiple datasets.

Since many performance scores are ratios of integers, it is common practice in scientific writing to round them to a finite number of decimal places for presentation. For instance, introducing the notation $\hat{v}_s$ for the reported figure of the score $s$, $\hat{v}_s = 0.945$ implies that the original value was rounded to 3 decimal places. Consequently, the true value $v_s^{*} = f_s(tp^{*}, tn^{*}, p, n)$ is expected to fall within the interval $v_s^{*} \in [\hat{v}_s - \epsilon, \hat{v}_s + \epsilon] = [0.9445, 0.9455]$, where $\epsilon = 10^{-3}/2$ represents the \emph{numerical uncertainty}. If flooring and ceiling are also allowed, the numerical uncertainty increases to $\epsilon = 10^{-3}$.

The problem we address in the rest of the paper can be phrased as follows: \emph{Given a set of reported performance scores and the description of the experiment (the datasets involved, the evaluation scheme, the mode of aggregation), could the experiment have yielded the reported scores?}

\begin{table*}
\caption{A summary of all performance scores discussed in the paper, including their standardized forms that depend on $tp$, $tn$, $p$, and $n$, their original definitions, and descriptions that mention common synonyms and complements.}
\label{tab:scores}
\begin{scriptsize}
\begingroup
\renewcommand{\arraystretch}{3.0}
\input{table_scores}
\endgroup
\end{scriptsize}
\end{table*}

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{flowchart/figure.pdf}
\end{center}
\caption{The intended use of the proposed methods.}
\label{fig:illustration}
\end{figure*}

\section{Testing scores derived from one confusion matrix}
\label{sec:ind}

In this section, we assume that there is a well-specified evaluation set available, with known numbers of positive ($p$) and negative ($n$) instances, and a set of scores $\mathcal{S}\subseteq\lbrace acc, sens, \dots\rbrace$ reported up to $\epsilon$ numerical uncertainty. 
It is worth noting that this experimental setup is common in various fields such as computer vision (when results for publicly available test images are shared \cite{vessel, isic2016}); big data (where the hold-out strategy is used to reduce computational demand); and machine learning competitions, including those on platforms like Kaggle (\url{www.kaggle.com}), where a closed test set is withheld.

This section is organized as follows: In Subsection \ref{sec:rommat}, we introduce the consistency test in terms of exhaustive search. In Subsection \ref{sec:improved}, we propose a more efficient implementation using interval computing. The method is illustrated through an example in Subsection \ref{sec:index}, and finally, limitations are discussed in Subsection \ref{sec:indpower}.

\subsection{Testing by exhaustive search}
\label{sec:rommat}

The experimental setup (described by $p$ and $n$) and the reported scores are consistent if there exist $tp^* \in \mathcal{P} \doteq \lbrace 0, \dots, p\rbrace$ and $tn^*\in\mathcal{N}\doteq \lbrace 0, \dots, n\rbrace$ integers such that
\begin{equation}
\label{eqtest0}
f_s(tp^*, tn^*, p, n) \in [\hat{v}_s - \epsilon, \hat{v}_s + \epsilon],
\end{equation}
holds for each score $s\in\mathcal{S}$ simultaneously. This simple condition readily suggests an $O(p\cdot n\vert S\vert)$ time complexity algorithm based on exhaustive search. One can test each pair of $(tp, tn)\in \mathcal{P}\times\mathcal{N}$ to see if they satisfy the conditions. If there are no feasible pairs, the experimental setup and the reported scores are inconsistent. Although this brute-force algorithm is functional and can be applied to datasets of even medium sizes ($p + n \lessapprox 10K$), it is possible to reduce its time complexity, making it efficient for much larger datasets.

\subsection{The improved time complexity test}
\label{sec:improved}

The idea behind the improvement is that the double iteration through the potential values of both $tp$ and $tn$ could be reduced to one if we could determine for a given value of $tp$ the values of $tn$ leading to a desired performance score $v_s^{*}$. To do so, the analytical forms of the score functions need to be solved for the particular variables. Namely, from $v_s = f_s(tp, tn, p, n)$ solving
\begin{equation}
v_s - f_s(tp, tn, p, n) = 0
\end{equation}
for $tn$ leads to the solution
\begin{align}
\label{eq:solution}
tn &= f_{s, tn}^{-1}(v_s, tp, p, n).
\end{align}
The solutions for all scores are provided in tables \ref{tab2} and \ref{tab3}.

If we knew the exact values of the scores $v_s^*$, one would need to check if there exists any $tp\in\mathcal{P}$ such that \break $f_{s, tn}^{-1}(v_s^*, tp, p, n)$ gives the same integer result for each $s\in\mathcal{S}$.

In practice, we have only interval estimations for $v_s^* \in [\hat{v}_s - \epsilon, \hat{v}_s + \epsilon]$. Therefore, to evaluate (\ref{eq:solution}) one needs to exploit interval arithmetic \cite{interval}. For example, given $p=40$, $n=70$ and $\hat{v}_{acc} = 0.927$, one wants to determine which $tn$ values could lead to the reported score if $tp=30$ (an arbitrary choice from $\mathcal{P}$). Selecting the solution $f^{-1}_{acc, tn}$ from Table \ref{tab2} and evaluating it with interval arithmetic yields
\begin{equation}
f^{-1}_{acc, tn}([0.926, 0.928], 30, 40, 70) = [71.86, 72.08],
\end{equation}
that is, the only integer $tn$ can take to yield the reported accuracy score is $tn=72$. We note that depending on the numerical uncertainty, the resulting intervals might contain multiple integers, and some scores have multiple solutions leading to the union of intervals (Table \ref{tab3}).

Ultimately, the consistency test can be carried out by iteratively testing if there exist at least one $tp\in\mathcal{P}$ such that the intersection $f_{s, tn}^{-1}([\hat{v}_s - \epsilon, \hat{v}_s + \epsilon], tp, p, n)$ for all $s\in\mathcal{S}$ contains at least one integer from the feasible set $\mathcal{N}$. If no such $tp\in\mathcal{P}$ exists, the experimental setup and the reported scores are inconsistent with each other. 
One can readily see that the choice of $tp$ to iterate by is arbitrary; the consistency test could be implemented by iterating through $tn\in\mathcal{N}$ and using solutions for $tp$ (also provided in Tables \ref{tab2} and \ref{tab3}). Consequently, the time complexity can be reduced to $O(\min(p, n)\cdot \vert S\vert)$ if the figure with the smaller domain is chosen for iteration, leading to an efficient, linear time algorithm, which is tractable even when the evaluation set contains millions of records. Finally, since dynamic data structures are not utilized, the space complexity of the algorithm becomes $O(1)$. The detailed pseudo-code of the test is listed in Algorithm \ref{alg1}. 

It is worth noting that the time complexity could be further reduced by solving pairs of performance scores as a system for $tp$ and $tn$, eliminating the need for iteration by $tp$ or $tn$. However, we found that solving pairs of scores with higher-order terms would require the involvement of advanced algebraic techniques, which falls beyond the scope of this paper.

\SetKwComment{Comment}{/* }{ */}

\begin{algorithm}[t]
    \caption{Consistency testing for scores computed directly from the confusion matrix}\label{alg1}
    \begin{small}
    \KwData{$p$, $n$, $\epsilon$, the set of scores reported $\mathcal{S}$, the reported values $\hat{v}_s$, $s\in\mathcal{S}$}
    \KwResult{$True$ if inconsistency was found, $False$ otherwise.}
    \Comment{Selecting the figure to solve for ($\beta$), the upper bound of the feasible set for $\beta$ ($B$), and the upper bound of the integer set to iterate on ($A$)}
    \eIf{$p < n$}{ 
    $\beta \gets \text{'tn'}$\;
    }{
    $\beta \gets \text{'tp'}$\;
    }
    $A \gets \min(p, n)$\;
    $B \gets \max(p, n)$\;
    \Comment{Iterate through the possible values}
    \For{$\alpha = 0, 1, \dots, A$}{
        $I \gets \bigcap\limits_{s\in\mathcal{S}} f^{-1}_{s, \beta}([\hat{v}_s-\epsilon, \hat{v}_s+\epsilon], \alpha, p, n)$\;
        \If{$I \bigcap \lbrace 0, 1, \dots, B\rbrace \neq \emptyset$}{
         \Comment{Evidence found for feasibility}
          \Return False\;
        }
    }
    \Comment{Inconsistency identified}
    \Return True\;
    \end{small}
    \end{algorithm}

\begin{table*}[t!]
\caption{Scores with single solutions.}
\label{tab2}
\begin{scriptsize}
\begin{center}
\input{table_solutions_0}
\end{center}
\end{scriptsize}
\end{table*}

\begin{table*}[t!]
\caption{Scores with multiple solutions.}
\label{tab3}
\begin{scriptsize}
\begin{center}
\input{table_solutions_1}
\end{center}
\end{scriptsize}
\end{table*}

\subsection{Example}
\label{sec:index}

Suppose there is an evaluation set of $p=1000$ positive and $n=6000$ negative samples, and the reported scores are $\hat{v}_{acc} = 0.6801$, $\hat{v}_{npv} = 0.9401$ and $\hat{v}_{f_1} = 0.4004$. Being conservative and assuming the scores are floored or ceiled, the numerical uncertainty is $\epsilon = 0.0001$. Applying Algorithm \ref{alg1}, one finds that there are two pairs of ($tp$, $tn$) values compatible with the setup: (743, 4031) and (743, 4032). If the scores were slightly adjusted, for example, accuracy changed to $0.6811$, Algorithm \ref{alg1} concludes that there is no ($tp$, $tn$) pair fulfilling all conditions. Similarly, if the assumption for $p$ is incorrect, for example, $p=1100$, Algorithm \ref{alg1} identifies the inconsistency.

\subsection{Limitations and power analysis}
\label{sec:indpower}

The proposed test is linear in the size of the evaluation set as well as in the number of reported scores; therefore, there should be no computational limitations with reasonably sized datasets. As the examples illustrate, the test is capable of recognizing inconsistencies in reported performance scores and presumed experimental setups. Naturally, smaller numerical uncertainty and more scores being reported both increase the sensitivity of the test. However, it is still unclear how sensitive the test really is to deviations from the assumptions. To address this question -- although the proposed method is numerical -- we draw an analogy with statistical hypothesis testing and utilize the existing terminology.

In the proposed consistency tests, the \emph{null hypothesis} is that the reported scores and the presumed experimental setup are consistent; the \emph{alternative hypothesis} is that they are inconsistent in some sense, that is, the scores are not calculated in the presumed way. Inconsistencies are identified with certainty; therefore, the probability of a \emph{type I error} (false rejection of the null hypothesis \cite{statdict}) is zero. However, the test can still make \emph{type II errors} \cite{statdict}, meaning that the null hypothesis is false (the scores are not calculated in the assumed way), but the test fails to recognize the inconsistency. The probability of making a type II error ($\beta$) is the complement of the \emph{power} ($1 - \beta$) of the test \cite{statdict}, which is assessed in \emph{power analysis} \cite{statdict} in the field of statistical hypothesis testing. Consequently, characterizing the sensitivity of the proposed test to recognize inconsistencies is equivalent to carrying out its power analysis.

Similar to statistical hypothesis testing, the power analysis requires making assumptions about the nature of the deviations (for example, typographical error in the last decimal place of a score; differing number of positive samples used to calculate the scores from those presumed, etc.) and also depends on the actual parameters (numerical uncertainty, the number of reported scores, etc.) of the problem. Therefore, similar to statistical hypothesis testing, a general power analysis cannot be carried out. However, it can be done in particular cases by simulations, similar to the one carried out in our previous paper addressing inconsistencies in the field of retinal vessel segmentation \cite{vessel} and in Subsection \ref{sec:third} of this paper.

\section{Testing scores derived by aggregations}
\label{sec:agg}

In this section, we develop tests for those scenarios where the scores are aggregated over multiple evaluation sets (folds and/or datasets). 
The mode of aggregation (discussed in Subsection \ref{sec:rommor}) leads to different tests, that we cover in Subsections \ref{sec:rom} and \ref{sec:mor}. 
The mapping of some common kFCV schemes to the representation used by the tests is discussed in Subsection \ref{sec:mapping}. Finally, limitations are discussed in Subsection \ref{sec:aggpower}.

\subsection{Mean of Scores and Score of Means aggregations}
\label{sec:rommor}

We assume an experiment involving $N_e$ evaluation sets with $p_i$ and $n_i$ positive and negative samples, respectively, for $i=1,\dots,N_e$, each leading to a separate confusion matrix with entries $tp_i \in \lbrace 0, \dots, p_i\rbrace$ and $tn_i\in \lbrace 0, \dots, n_i\rbrace$. We are concerned about how these figures are summarized by scalar scores for the entire experiment. 

A natural way of aggregation is to calculate a score for each evaluation set separately and take the average. Formally, for a particular score $s$, the overall score is calculated as
\begin{equation}
\label{estmor}
v_s^{MoS} = \dfrac{1}{N_e}\sum\limits_{i=1}^{N_e} f_s(tp_{i}, tn_{i}, p_{i}, n_{i}),
\end{equation}
where we introduced the notion of \emph{Mean of Scores} (MoS) to indicate the way of aggregation. We note that the MoS mode of aggregation is extremely common in the evaluation of binary classifiers in cross-validation scenarios, with the benefit that the $N_e$-sized sample of scores enables the estimation of confidence intervals \cite{morex2} and the use of hypothesis testing for the comparison of classification techniques \cite{morex0}.

Alternatively, one can calculate the averages of the $tp$, $tn$, $fp$, and $fn$ figures first, for example, $\overline{tp} = \dfrac{1}{N_e}\sum\limits_{i=1}^{N_e} tp_i$, and compute the scores from the mean figures as
\begin{equation}
\label{estrom}
v_s^{SoM} = f_s\left(\overline{tp}, \overline{tn}, \overline{p}, \overline{n}\right),
\end{equation}
where we introduced the notion of \emph{Score of Means} (SoM) to reflect the way of aggregation. One can readily see that the SoM aggregation is equivalent to a weighted MoS aggregation when the weights are defined as the denominators of the scores. SoM aggregation is beneficial when the scores for some individual evaluation sets might become undefined, typically with small and imbalanced data \cite{romex0} (for example, if a fold has only a handful of positive samples, $tp=0$ and $fp=0$ lead to an undefined positive predictive value, which is a less likely scenario for $\overline{tp}$ and $\overline{fp}$).

The terms used for the aggregations are inspired by the analogous concepts of \emph{Ratio of Means} (RoM) and \emph{Mean of Ratios} (MoR) estimations for ratio statistics \cite{rommor, rommor2}, but generalized to accommodate the non-linearities in the numerators and denominators of some scores (such as Matthews correlation coefficient).

From the theoretical point of view, the goal of using multiple evaluation sets and aggregating the results is to get a more reliable estimation of performance for the population of problems represented by the evaluation sets. (We note that this concept leads to difficulties when multiple datasets are involved, as the population of classification problems represented by some datasets is usually not well-defined \cite{nfl}.) Nevertheless, estimation theory \cite{estimationtheory} can be expected to provide a guideline on which aggregation is more reasonable. Interestingly, already for the simplest scores (with linear terms in the numerator and denominator) it turns out that both aggregation schemes (\ref{estmor}) and (\ref{estrom}) are biased estimators of the population level statistics \cite{rommor2}. Moreover, even in the same experiment, different scores can lead to different interpretations regarding their meaning. For example, in kFCV, if the total number of samples ($p + n$) is divisible by the number of folds, the fold-level accuracy scores have the same constant denominator, the MoS and SoM aggregations become the same, and the aggregated accuracy becomes an unbiased estimator of the population level proportion of correctly classified items. In the same scenario, sensitivity has randomness in the denominator since the various folds can have varying number of positive samples; consequently, one can argue that weighting by the number of positive samples in a fold (using SoM) is a meaningful way to reduce noise. Finally, positive predictive value has correlated randomness in its numerator and denominator (through the presence of $tp$), leading to both the SoM and MoS aggregations becoming biased estimators \cite{rommor2}. Consequently, there is no consensus on the superiority of either mode of aggregation.

In practice, when the data distribution across evaluation sets is fairly uniform, the scores calculated by both aggregations are nearly identical (see Table \ref{mossomex}). Therefore, authors often do not explicitly describe the method of aggregation, as it is not expected to significantly alter the qualitative outcome of the research. A particular choice could be motivated by multiple factors, for example: the best practices of a field, the available implementation, the need to estimate the uncertainty of the scores, small and imbalanced data, etc. Even in the same domain, with the same data, one can find examples of both aggregations \cite{vessel}. Therefore, unless explicitly phrased, one cannot assume any aggregation as default.

Since we develop sharp tests to check the consistency of reported scores, even minor differences must be handled with mathematical rigor. Therefore, in Subsections \ref{sec:rom} and \ref{sec:mor}, we develop consistency tests for the two types of aggregations separately.

\begin{table*}
\caption{Comparison of the MoS and SoM evaluations on sample data in a k-fold cross-validation scenario with $k=5$: the folds and a sample evaluation are shown in Table \ref{tab4a}, and the scores in Table \ref{tab4b}. As expected, the more non-linearities are present in a score, the more the two aggregations deviate, but the most commonly used scores (accuracy, sensitivity, specificity, $f^1_+$) are very close to each other.}
\label{mossomex}
\begin{center}
\begin{footnotesize}
\subfloat[(a)][The folds.\label{tab4a}]{
\begin{tabular}[t]{r@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{5pt}}r}
\toprule
fold ($i$) & $p_i$ & $n_i$ & $tp_i$ & $tn_i$ \\
\midrule
0 & 100 & 201 & 78 & 189 \\
1 & 100 & 200 & 65 & 191 \\
2 & 100 & 200 & 81 & 160 \\
3 & 101 & 200 & 75 & 164 \\
4 & 101 & 200 & 72 & 171 \\
\bottomrule
\end{tabular}
}
\subfloat[(b)][The scores calculated by the two aggregation techniques.\label{tab4b}]{
\begin{tabular}{l@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{5pt}}|l@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{5pt}}|l@{\hspace{5pt}}r@{\hspace{5pt}}r@{\hspace{5pt}}|l@{\hspace{5pt}}r@{\hspace{5pt}}r}
\toprule
score & MoS & SoM & score & MoS & SoM & score & MoS & SoM & score & MoS & SoM \\
\midrule
acc & 0.8290 & 0.8290 & $f^1_+$ & 0.7443 & 0.7427 & lrn & 0.2975 & 0.2985 & ppv & 0.7606 & 0.7465 \\
bacc & 0.8066 & 0.8066 & fm & 0.7471 & 0.7428 & lrp & 8.1202 & 5.8713 & pt & 0.2795 & 0.2921 \\
bm & 0.6131 & 0.6132 & gm & 0.8021 & 0.8038 & mcc & 0.6215 & 0.6147 & sens & 0.7391 & 0.7390 \\
dor & 28.0174 & 19.6671 & ji & 0.5945 & 0.5908 & mk & 0.6312 & 0.6163 & spec & 0.8741 & 0.8741 \\
$f^1_-$ & 0.8709 & 0.8719 & kappa & 0.6165 & 0.6147 & npv & 0.8706 & 0.8698 & upm & 0.8025 & 0.8022 \\
\bottomrule
\end{tabular}
}
\end{footnotesize}
\end{center}
\end{table*}

\subsection{Testing scores aggregated by the \emph{Score of Means} approach}
\label{sec:rom}

While we introduced the term \emph{Score of Means} to reflect the analogy with the concept of \emph{Ratio of Means} in statistics, taking the mean of the figures $tp$, $tn$, $p$, and $n$ (equation \ref{estrom}) is unnecessary. It can be readily seen that all scores covered in the paper (see Table \ref{tab:scores}) are invariant to scaling. In other words, for any score $s$, the equation $f_s(tp, tn, p, n) = f_s(\alpha\cdot tp, \alpha\cdot tn, \alpha\cdot p, \alpha\cdot n)$ holds for $\alpha \in\mathbb{R}^{+}$, and consequently,
\begin{equation}
f_s(\overline{tp}, \overline{tn}, \overline{p}, \overline{n}) = f_s\left(\sum\limits_{i=1}^{N_e} tp_i, \sum\limits_{i=1}^{N_e} tn_i, \sum\limits_{i=1}^{N_e} p_i, \sum\limits_{i=1}^{N_e} n_i\right).
\end{equation}
Therefore, any score calculated using the SoM approach can be treated as if it were calculated from the confusion matrix of a problem with $p'=\sum\limits_{i=1}^{N_e} p_i$ positive and $n'=\sum\limits_{i=1}^{N_e} n_i$ negative samples. Consequently, when scores aggregated in the SoM manner are reported, the consistency tests developed in Section \ref{sec:ind} are applicable using the total number of positives $p'$ and negatives $n'$.

\subsection{Testing scores aggregated by the \emph{Mean of Scores} approach}
\label{sec:mor}

In this section, we develop consistency tests for the MoS aggregation. First, we formulate the problem mathematically in Subsection \ref{sec:mosmath}, then propose a tractable algorithm based on integer linear programming in Subsection \ref{sec:moslinprog}, and finally, illustrate the use of the technique in Subsection \ref{sec:mosex}.

\subsubsection{Mathematical formulation}
\label{sec:mosmath}
According to the definition of MoS aggregation (equation (\ref{estmor})), we are concerned with the simultaneous feasibility of the inequalities:
\begin{equation}
\label{nlopt}
\hat{v}_s^{MoS} - \epsilon \leq \dfrac{1}{N_e}\sum\limits_{i=1}^{N_e} f_s(tp_i, tn_i, p_i, n_i) \leq \hat{v}_s^{MoS} + \epsilon, \; \text{for } s \in \mathcal{S},
\end{equation}
where $tp_i \in \lbrace 0, \dots, p_i\rbrace$ and $tn_i \in \lbrace 0, \dots, n_i\rbrace$ for $i = 1, \dots, N_e$. Due to the non-linearities and the presence of raw figures $tp_i$ and $tn_i$ in both the numerators and denominators of most scores, the averaging cannot be simplified, resulting in a total of $2N_e$ degrees of freedom in the general case.

Similarly to the approach introduced in Subsection \ref{sec:rommat}, one could enumerate all possible combinations of the $0 \leq tp_i \leq p_i$ and $0 \leq tn_i \leq n_i$, $i=1, \dots, N_e$ figures and check if any of them leads to the reported scores $\hat{v}_s^{MoS}$, $s\in\mathcal{S}$ within the numerical uncertainty $\epsilon$. However, the time complexity $O\left(\prod_{i=1}^{N_e}p_in_i\right)$ of this brute-force approach renders it intractable in practice, even in the simplest cases: a 5-fold evaluation with $p_i\sim 10$ positive and $n_i\sim 10$ negative records in each fold lead to approximately $10^{10}$ different combinations of the free parameters. 

\subsubsection{Feasibility by integer linear programming}
\label{sec:moslinprog}

The condition set (\ref{nlopt}) can be interpreted as the definition of the feasibility region of a non-linear integer programming problem (with any dummy objective function). In general, non-linear integer programming is NP-complete \cite{ip}, with no efficient algorithms for exact solutions, and approximations are not suitable for sharp consistency tests requiring exact decisions regarding feasibility. 

However, for that subset of scores which leads to linear conditions, integer linear programming can be exploited, which is solvable by numerous techniques \cite{ip}. Consequently, the proposed consistency test for MoS aggregations supports only those scores which are linear functions of $tp$ and $tn$, namely, \emph{accuracy}, \emph{sensitivity}, \emph{specificity}, and \emph{balanced accuracy}. Although this test is limited to these four scores only, we note that these scores are among the most commonly reported ones. 
Expanding (\ref{nlopt}) for the linear scores leads to the condition set
\begin{align}
\label{aggtest}
\hat{v}_{acc}^{MoS} - \epsilon & \leq \dfrac{1}{N_e} \sum\limits_{i=1}^{N_e} \dfrac{tp_i + tn_i}{p_i + n_i} \leq \hat{v}_{acc}^{MoS} + \epsilon, \nonumber \\
\hat{v}_{sens}^{MoS} - \epsilon & \leq \dfrac{1}{N_e} \sum\limits_{i=1}^{N_e} \dfrac{tp_i}{p_i} \leq \hat{v}_{sens}^{MoS} + \epsilon, \nonumber \\
\hat{v}_{spec}^{MoS} - \epsilon & \leq \dfrac{1}{N_e} \sum\limits_{i=1}^{N_e} \dfrac{tn_i}{n_i} \leq \hat{v}_{spec}^{MoS} + \epsilon, \nonumber \\
\hat{v}_{bacc}^{MoS} - \epsilon & \leq \dfrac{1}{N_e} \sum\limits_{i=1}^{N_e} \dfrac{tp_i}{2p_i} + \dfrac{tn_i}{2n_i} \leq \hat{v}_{bacc}^{MoS} + \epsilon, \nonumber \\
tp_i &\in \lbrace 0, \dots, p_i\rbrace, \quad tn_i \in \lbrace 0, \dots, n_i\rbrace,
\end{align}
which is the most general set of conditions that is compatible with integer integer programming. The consistency test operates by specifying the conditions (\ref{aggtest}) for the available scores $\mathcal{S} \bigcap \lbrace acc, sens, spec, bacc\rbrace$, and using any integer linear programming solver to check the feasibility of the condition set. If the conditions are feasible, there is no inconsistency between the scores and the experimental setup; if the feasibility region is empty, the reported scores and the assumptions on the experimental setup are inconsistent.

We mention that there is one more piece of information that is sometimes reported and can strengthen the test: the minimum and maximum scores across folds. As noted in Subsection \ref{sec:rommor}, one benefit of using MoS aggregation is that one gets a distribution of scores, and sometimes authors report the minimum and maximum values achieved across the folds. Adding these constraints shrinks the feasibility region and improves the sensitivity of the test. For example, if minimum ($\hat{v}_{min(acc)}^{MoS}$)  and maximum ($\hat{v}_{max(acc)}^{MoS}$) scores are reported for accuracy, additional $N_e$ pieces of constraints can be added to the linear programming problem:

\begin{align}
\hat{v}_{min(acc)}^{MoS} - \epsilon \leq \dfrac{tp_i + tn_i}{p_i + n_i} \leq \hat{v}_{max(acc)}^{MoS} + \epsilon, i = 1, \dots, N_e
\end{align}

We note that under special circumstances (stratified kFCV, both $p$ and $n$ divisible by the number of folds), for some subsets of the scores possibly fractional or convex programming with certain relaxation techniques could be exploited \cite{nonlinear}. However, the exploration of these special cases is beyond the scope of the paper.

Finally, the time complexity of the test is equivalent to that of integer linear programming, which is known to be NP-hard \cite{ip}. However, due to the relatively small number of folds used in typical experiments, the problems to be solved are typically small, and the test remains tractable in practice. The space complexity of integer linear programming depends on the algorithm implemented by a particular solver.

\subsubsection{Example}
\label{sec:mosex}

The usage of the test is illustrated through the sample problem shared in Table \ref{mossomex}. Suppose the scores
\begin{equation}
\hat{v}_{acc}^{MoS} = 0.8290, \quad
\hat{v}_{sens}^{MoS} = 0.7391, \quad
\hat{v}_{spec}^{MoS} = 0.8741
\end{equation}
are reported. With a conservative choice of numerical uncertainty set at $\epsilon=0.0001$ (allowing ceiling or flooring), after substituting the scores and the fold specifications from Table \ref{tab4a} into (\ref{aggtest}), and testing the feasibility using an integer linear programming solver (specifically, we used the Python package \verb|pulp| \cite{pulp}), the solver confirms that the problem is feasible. This result suggests that the reported scores could indeed have been obtained from the experiment. However, if accuracy is incorrectly reported as $\hat{v}_{acc}^{MoS} = 0.8280$, the solver returns that the configuration is infeasible, indicating that the scores are incompatible with the assumptions on the experiment.

\subsection{Application of the tests in various experimental setups}
\label{sec:mapping}

In the previous sections, we introduced consistency tests for the SoM and MoS modes of aggregation. In this section, we discuss the assessment of various kFCV schemes using these tests. In a kFCV experiment with \emph{k} folds on a dataset comprising \emph{p} positive and \emph{n} negative entries, we define the \emph{fold configuration} as the distribution of positives and negatives across the \emph{k} folds, denoted as $(p_i, n_i)_{i=1}^{k}$. As usual in kFCV, we assume $p_i + n_i$ is either $\lfloor (p + n) / k\rfloor$ or $\lfloor (p + n) / k\rfloor + 1$. Additionally, we require that at least two folds contain at least one negative and at least two folds contain at least one positive sample. This ensures that all training sets in the folding process contain samples of both classes, and at least accuracy is calculable in each iteration. We treat fold configurations as multisets, where a certain pair of positive and negative counts can appear multiple times. The order of the pairs is irrelevant, as they lead to the same linear programming problem regardless of order.

%, allowing the computation of at least the accuracy score is computable on each fold.

\subsubsection{Testing the SoM scores of kFCV experiments}
\label{sec:somkfcv}

In the case of SoM aggregation, as discussed in Subsection \ref{sec:rom}, only the overall counts of positive and negative samples are necessary for testing. Therefore, in a standard kFCV experiment (where each entry of a dataset is evaluated once), the parameters $p$ and $n$ of the dataset should be used. Generally, in a repeated kFCV scenario involving $N_d$ datasets with $N_r$ repetitions, since the fold configurations are irrelevant, the parameters $p'=N_r\cdot \sum\limits_{i=1}^{N_d} p_i$ and $n'=N_r\cdot \sum\limits_{i=1}^{N_d} n_i$ need to be used for testing.

\subsubsection{Testing the MoS scores of kFCV experiments}
\label{sec:moskfcv}

For MoS aggregations, knowing the fold configuration is crucial to formulate the linear programming problem (\ref{aggtest}). While some data providers, such as the KEEL data repository \cite{keel}, supply foldings of the datasets, the fold configuration is typically unknown in general kFCV scenarios. However, there are special cases where the fold configuration can be inferred. Particularly, when \emph{stratified KFCV} is used, proper stratification methods (such as the technique implemented in the \emph{sklearn} \cite{sklearn} Python package) ensure that each fold differs by at most one sample in terms of the overall count of items, as well as the counts of positives and negatives. This characteristic implies a unique fold configuration that can be determined from $p$, $n$ and $k$ as shown in Table \ref{tab:strfolds}. In repeated kFCV experiments or when using multiple datasets, the joint fold configuration needs to be used.

\begin{table*}
    \caption{Fold configurations by the stratified kFCV implemented in \emph{sklearn}, with $p_{mod} = p \mod k$, $p_{div} = \lfloor p/k\rfloor$, $n_{mod} = n \mod k$, $n_{div} = \lfloor n/k\rfloor$. The \emph{count of folds} column indicates how many times folds with $p_i$ and $n_i$ counts appear in the configuration, when $p_{mod} + n_{mod} > k$ (a) and $p_{mod} + n_{mod} \leq k$ (b).}
    \label{tab:strfolds}
\begin{center}
\begin{small}
 \begin{tabular}{cc}   
 %   \begin{center}
 %   \begin{small}
    \subfloat[][If $p_{mod} + n_{mod} > k$]{
    \begin{tabular}{ccc}
    \toprule
    count of folds & positives ($p_i$) & negatives ($n_i$) \\
    \midrule
    $p_{mod} + n_{mod} - k$ & $p_{div} + 1$ & $n_{div} + 1$\\
    $k - n_{mod}$ & $p_{div} + 1$ & $n_{div}$\\
    $k - p_{mod}$ & $p_{div}$ & $n_{div} + 1$\\
    \bottomrule
    \end{tabular}
    }
    &
    \subfloat[][If $p_{mod} + n_{mod} \leq k$]{
    \begin{tabular}{ccc}
    \toprule
    count of folds & positives ($p_i$) & negatives ($n_i$) \\
    \midrule
    $k - p_{mod} - n_{mod}$ & $p_{div}$ & $n_{div}$\\
    $p_{mod}$ & $p_{div} + 1$ & $n_{div}$\\
    $n_{mod}$ & $p_{div}$ & $n_{div} + 1$\\
    \bottomrule
    \end{tabular}
   }
\end{tabular}
 \end{small}
 \end{center}
\end{table*}

\subsubsection{Testing in the lack of knowing the fold configuration}
\label{sec:kfold}

When stratification is not used or its use is not explicitly indicated in a paper, any fold configuration can be assumed. Addressing these scenarios requires enumerating and testing all possible fold configurations. Although this might seem intractable, it is feasible even in real-life applications, as demonstrated in Subsection \ref{sec:ehg}, especially when the number of folds falls in the usual range (5-10) and the dataset is imbalanced or relatively small. If all configurations prove inconsistent with the reported scores, it can be concluded that the scores could not have been yielded from the assumed experiment. Enumerating all possible k-fold configurations given $p$, $n$, and $k$ is a non-trivial combinatorial problem. In the remainder of this section, we develop an algorithm tailored specifically for this task.

The proposed algorithm is based on the observation that enumerating all fold configurations is closely related to the problem of integer partitioning in combinatorics and number theory \cite{intpart}. Specifically, we are interested in the various ways $p$ (or $n$) can be decomposed as $p = p_1 + \dots + p_k$. Given a particular partition, it can be complemented with negatives to achieve the desired cardinality of folds ($n_i \sim (p + n)/k - p_i$), resulting in one fold configuration. There are algorithms proposed for the enumeration of all unique partitions of an integer to $m$ positive parts (for example, the algorithm on page 343 in \cite{intpart}).
The main complexity lies in addressing the varying cardinalities of folds when the total number of elements ($p + n$) is not divisible by the number of folds ($k$).
%The only complexity that needs to be addressed is that the folds have varying cardinalities if the total number of elements ($p + n$) is not divisible by the number of folds ($k$).

Let $k_{div} = \lfloor (p + n) / k \rfloor$ and $k_{mod} = (p + n) \mod k$. There are two types of folds regarding cardinalities, denoted by superscripts $a$ and $b$: $k^a = k_{mod}$ folds, each with $c^{a} = k_{div} + 1$ elements, and $k^b = \defeq k - k_{mod}$ folds each with $c^b = k_{div}$ elements. Without the loss of generality, we choose the number of positives to drive the enumeration. Suppose there are overall $p^a$ positive samples in the folds of type $a$, implying $p^b = p - p^a$ positive samples in the folds of type $b$. The various configurations of $p^a$ positives distributed in the folds of type $a$ can be determined by enumerating all integer partitions of $p^a$ into at most $k^a$ parts. Similarly, for folds of type $b$, one can determine all integer partitions of $p^b$ into at most $k^b$ parts. One combination of the partitions of positives in folds of type $a$ and $b$ can be complemented by negative samples to match the cardinality of the respective folds, resulting in a unique fold configuration. By iterating through all possible ways to split the total number of positives $p$ between the two types of folds, all fold configurations can be generated. A precise algorithm (with all technical details such as the removal of configurations not having a certain class present in at least two folds) is provided in Algorithm \ref{alg2}. We note that in practice, depending on the scores reported, some configurations can be skipped. For instance, if sensitivity is reported, configurations with folds having zero positives lead to undefined sensitivity scores, contradicting the fact that the average sensitivity is reported.

\begin{algorithm}[t]
\caption{The algorithm generates all possible fold configurations, ensuring that for each class there at least 2 folds containing at least one sample, thereby ensuring a valid training set in all configurations. $\mathbf{0}_{x}$ and $\mathbf{1}_{x}$ denote the $x$ dimensional 0-vector and 1-vector, respectively, $[\mathbf{x}, \mathbf{y}]$ stands for the concatenation of vectors $\mathbf{x}$ and $\mathbf{y}$.}\label{alg2}

\SetAlFnt{\small}
\SetKwFunction{FKFoldConfigurations}{KFoldConfigurations}
\SetKwProg{Fn}{Function}{:}{}
\SetKwProg{Gen}{Generator}{:}{}
\SetKwFunction{FMPartition}{MPartition}
\SetKwFunction{FAllPartitions}{Partition}
\SetKwFunction{FInvalidPCounts}{InvalidPCounts}
\SetKwFunction{FPRange}{PRange}
\SetKwFunction{FDistribution}{Distribution}
\SetKwFunction{FCombination}{Combinations}
\SetKwInOut{Yield}{Yield}
\SetKw{Args}{Args:}
\SetKw{Yields}{Yields:}
\SetKw{Description}{Description:}

\Gen{\FKFoldConfigurations{$p$, $n$, $k$}}{
\Description{Generates all fold configurations.}\\
\Args{the number of positives ($p$), negatives ($n$), and folds ($k\geq 2$)}\\
\Yields{$(\mathbf{p}, \mathbf{n}) \in \mathbb{N}^{k\times k}$, $(\mathbf{p}_i, \mathbf{n}_i)$ representing the number of positives and negatives in the $i$th fold, respectively.}

$k_{div}, k_{mod} \gets \lfloor (p + n) / k \rfloor, (p + n) \mod k$\;

$k^a, k^b \gets k_{mod}, k - k_{mod}$\;
$c^a, c^b \gets k_{div} + 1, k_{div}$\;

\For {$p^{a} = 0, \dots, \min\lbrace p, k^a\cdot c^a\rbrace$}{
    \For{$\mathbf{p}^a$ in \FAllPartitions{$p^a$, $k^a$, $c^a$}}{
        \For{$\mathbf{p}^b$ in \FAllPartitions{$p - p^a$, $k^b$, $c^b$}}{
            $\mathbf{p} \gets [\mathbf{p}^a, \mathbf{p}^b$]\;
            $\mathbf{n} \gets [\mathbf{1}_{k^a}\cdot c^a - \mathbf{p}^a, \mathbf{1}_{k^b}\cdot c^b - \mathbf{p}^b$]\;
            \If{$\sum\limits_{i=1}^{k} \mathbb{I}_{\mathbf{p}_i > 0} \geq 2 \wedge \sum\limits_{i=1}^{k}\mathbb{I}_{\mathbf{n}_i > 0} \geq 2$}{
            \Yield{$\mathbf{p}, \mathbf{n}$}
            }
        }
    }
}
}

\Gen{\FAllPartitions{$q$, $k$, $q_{max}$}}{
\Description{Generates all partitions of $q$ with at most $k$ parts, no part greater than $q_{max}$.}\\
\If{$q = 0$}{\Yield{$\mathbf{0}_k$}}
\For{$m = 1, \dots, \min(q, k)$ }{
\For{$\mathbf{q}$ in \FMPartition{q, m}}{
\If{$\sum\limits_{i=1}^{m}\mathbb{I}_{\mathbf{q}_i > q_{max}} = 0$}{
\Yield{$[\mathbf{0}_{k - m}, \mathbf{q}]$}
}
}
}
}

\Gen{\FMPartition{$q$, $m$}}{
\Description{Implements the algorithm on page 343 in \cite{intpart}, iteratively yielding one unique partitioning of $q$ to $m$ parts: $\mathbf{q}\in\mathbb{Z}_{+}^{m}$.}
}

\end{algorithm}

For instance, in the case of $p=30$, $n=300$, and $k=5$ (which is comparable in size to many small and imbalanced medical datasets), the total number of fold configurations is $673$. As an example, one particular output yielded by the generator in Algorithm \ref{alg2} is a pair of vectors $\mathbf{p}=[1, 2, 6, 9, 12], \mathbf{n} = [65, 64, 60, 57, 54]$ representing the fold configuration $[(p_1=1, n_1=65), (p_2=2, n_2=64), (p_3=6, n_3=60), (p_4=9, n_4=57), (p_5=12, n_5=54)]$.

\subsubsection{Testing in the lack of knowing the mode of aggregation}
\label{sec:lackagg}

If mode of aggregation (MoS or SoM) is unknown, one can still apply the proposed consistency tests to identify inconsistencies: the scores can be tested under both assumptions. If both tests lead to inconsistencies, it can be concluded that the reported scores could not have resulted from the experimental setup under either of these two reasonable modes of aggregation.

\subsection{Limitations and power analysis}
\label{sec:aggpower}

The tests for aggregated scores rely on integer linear programming and inherit the limitations of the available solvers. Since the number of free variables is twice the number of evaluation sets (see eq. (\ref{aggtest})), the integer linear programs implied by typical 5- or 10-fold cross-validation schemes (10-20 free variables) are usually tractable. However, using many more evaluation sets can deteriorate the solvability of the system. Naturally, smaller numerical uncertainty and more reported scores both improve the ability of the test to recognize inconsistencies.

The analogy drawn between the proposed tests and statistical hypothesis testing in Subsection \ref{sec:indpower} is equally valid for testing aggregated scores. Assessing the sensitivity of the test to recognize inconsistencies requires a power analysis, which cannot be conducted without making assumptions about the nature of the deviations. However, in specific cases, it can be done through numerical simulations, similar to the analyses carried out in our previous paper \cite{vessel} addressing inconsistencies in the field of retinal vessel segmentation, and also in Subsection \ref{sec:ehg} of this paper.

\section{Applications}
\label{sec:applications}

In this section, we demonstrate the application of the proposed consistency tests in three real problems related to the use of machine learning in medicine and also discuss potential further applications.

\subsection{Retinal vessel segmentation and further applications in retina image processing}
\label{sec:retina}

The techniques proposed in this paper are generalizations of those used in our previous work \cite{vessel} in the field of retinal vessel segmentation. In this subsection, we provide a concise overview of that scenario, including the results and potential applications in related fields. 
The field of retinal vessel segmentation has been a popular research area for nearly two decades. The most widely used dataset for evaluation (DRIVE \cite{drive}) offers 20 training and 20 test images with manual annotations. Due to the well-defined test set, the reported performance scores -- typically accuracy, sensitivity, and specificity -- serve as the basis for ranking algorithms in most papers. 
Due to the specialized image acquisition techniques, the useful image content resides in a disk-shaped area in the center of a rectangular image, referred to as the \emph{Field of View} (FoV) (see Figure \ref{fig:retina} for one entry of the DRIVE dataset). Textual evidence suggests that some authors evaluate the segmentation performance within the FoV region only, while others use all pixels in the images. However, in most papers, the region of evaluation is not specified explicitly. The problem arises from the fact that the pixels outside the FoV region can easily be identified as non-vessel pixels, and account for about 30\% of all pixels. Including these pixels as true negatives boosts the accuracy and specificity scores compared to evaluating only the pixels within the FoV region. 

For a fair comparison and ranking of algorithms, it is desired to identify the evaluation methodology used for calculating a certain set of scores. This can be achieved by the proposed methods (Sections \ref{sec:ind}, \ref{sec:rom} and \ref{sec:mor}) by testing the consistency of the scores under two assumptions:
\begin{itemize}
    \item The authors used only the pixels in the FoV region for evaluation;
    \item The authors used all pixels of the images for evaluation (note that the overall number of negatives, $n$, is different). 
\end{itemize}

%In the literature, a minority of authors report performance scores at the image level, while the majority only provide average accuracy, sensitivity, and specificity scores based on the 20 test images. 

In our meta-analysis \cite{vessel}, we selected the 100 most cited papers in the field and assessed the consistency of both image-level and aggregated scores under the two assumptions, uncovering the following key insights into the state of the art in the field:
\begin{enumerate}
    \item Approximately 30\% of the papers reported scores that are inconsistent with both assumptions, casting doubt on their validity.
    \item For the remaining papers, we identified the evaluation region, and revealed a systematic bias: authors using all pixels of the images reported higher performance scores -- an artifact of the undisclosed evaluation methodology -- resulting in skewed algorithm rankings favoring those using all pixels for evaluation. 
    \item In 100 of the most cited papers in the field, incomparable performance scores were compared and ranked regardless of the evaluation region, leading to biased conclusions about the capabilities of certain algorithms.
\end{enumerate}


Numerous other lesions and anatomical structures in retinal images, such as \emph{exudates} \cite{exu} and the \emph{optic disk} \cite{od}, are targeted by segmentation and detection algorithms. The possibility of evaluating their performance in the FoV region or using all pixels in the images exists in all these problems. Building on the successful application of the proposed techniques for retinal vessel segmentation, it is reasonable to assume that these methods can be applied to validate and rectify the reported results in other problems of retinal image processing.

\begin{figure}[t]
\begin{center}
\subfloat[(a)][Retina image\label{retimg}]{
\includegraphics[width=0.31\textwidth]{21_training.png}
}
\subfloat[(b)][Vessel mask\label{retves}]{
\includegraphics[width=0.31\textwidth]{21_manual1.png}
}
\subfloat[(c)][Field of View mask\label{retfov}]{
\includegraphics[width=0.31\textwidth]{21_training_mask.png}
}
\end{center}
\caption{One entry ("21") of the DRIVE retinal vessel segmentation dataset: a retina image (\ref{retimg}); the vessel mask (white pixels indicate the vasculature to be segmented) (\ref{retves}); and the FoV mask, white pixels indicating the FoV region (\ref{retfov}).}
\label{fig:retina}
\end{figure}
    
\subsection{Preterm delivery prediction from electrohysterogram signals}
\label{sec:ehg}

In recent years, there has been growing interest in predicting preterm delivery from electrohysterogram (EHG) signals \cite{ehgreview}, particularly with the availability of the TPEHG dataset \cite{tpehg}, containing 38 positive and 262 negative records.
At some point, multiple authors reported almost perfect prediction scores in kFCV scenarios. However, in the study \cite{ehg}, it was revealed that these exceptionally high performance scores could not be replicated. The root cause of these overly optimistic results was traced to a methodological flaw: the improper use of minority oversampling.

The Synthetic Minority Oversampling TEchnique (SMOTE) \cite{smote} and its variations are commonly employed techniques to enhance the performance of binary classification on highly imbalanced data \cite{add2}. These techniques involve artificially generating additional training samples for the minority class to address the asymmetric degeneracy in the learning process. When employing minority oversampling in a kFCV scenario, it is critical to apply oversampling to each training set separately, excluding any elements of the fold designated for testing. Applying oversampling to the entire dataset prior to kFCV adds highly correlated samples to the dataset, leading to a significant data leakage. The authors of \cite{ehg} reproduced all of 11 studies and concluded that the most likely cause for the overly optimistic results is the application of minority oversampling prior to kFCV.

When minority oversampling is applied prior to kFCV, it increases the number of samples used for evaluation. Therefore, the scores derived from the augmented dataset can be expected to exhibit inconsistencies with the correct experimental setup. Consequently, the time-consuming and error-prone task of reimplementing algorithms to verify the results (as the authors did in \cite{ehg}) could be replaced by employing the techniques developed in this paper. For example, one study with the methodological flaw \cite{ehgflaw2} reported the scores $\hat{v}_{acc}^{MoS} = 0.9447$, $\hat{v}_{sens}^{MoS} = 0.9139$ and $\hat{v}_{spec}^{MoS} = 0.9733$ in a 5-fold cross-validation scenario. Although the authors mentioned using minority oversampling to increase the overall number of positive samples to $p'=244$, it remains unclear whether they used the additional samples solely for training or also for evaluation.
The authors did not mention using stratification, hence testing all possible fold configurations is necessary for conclusive results. 
With the parameters of the correct experimental setup ($p=38$, $n=262$, and $k=5$), the number of possible fold configurations becomes 918 (determined by the exhaustive enumeration algorithm presented in Subsection \ref{sec:kfold}).
Applying the MoS test (Subsection \ref{sec:mor}) reveals inconsistencies between the reported scores and each of the 918 configurations, suggesting that the reported scores could not have been obtained through 5-fold cross-validation on the original dataset.
Under the assumption of using $p'=244$ positive samples (i.e., including the highly correlated generated samples for evaluation), the total number of possible fold configurations expands to approximately $\sim2.6M$. Notably, the 962nd fold configuration $[(1, 101), (4, 97), (40, 61), (99, 2), (100, 1)]$ already provides evidence that the reported scores could be achieved with the $p'$ and $n$ counts and 5-fold cross-validation, with the corresponding $(tp_i, tn_i)$ counts of $[(1, 96), (3, 92), (38, 59), (90, 2), (96, 1)]$. With this example, we have provided numerical confirmation of the findings in \cite{ehg} regarding the improper use of minority oversampling in \cite{ehgflaw2}, without the need to reimplement \cite{ehgflaw2}.

The power analysis of the method in this particular problem can be conducted as follows. Firstly, we specify the type of inconsistencies for which we are performing the power analysis. In this case, a reasonable choice is inconsistencies arising from the application of minority oversampling prior to 5-fold cross-validation. Specifically, the reported accuracy, sensitivity and specificity scores are calculated from a dataset with $p=262$ and $n=262$ using 5-fold cross-validation.
We simulate such scenarios by randomly drawing the number of true positives and true negatives for each fold, calculating the performance scores, averaging them, and rounding to $k$ decimal places. Subsequently, we apply the proposed consistency test assuming the proper experimental setup ($p=38$) and record whether inconsistencies are detected. 
Based on 1000 such test scenarios, we estimate the probability of detecting this specific type of inconsistency when the performance scores are rounded to 2, 3, or 4 decimal places. The results are summarized in Figure \ref{fig:powertpehg}, where the vertical axis represents the power of the test (probability of recognizing the inconsistency). As depicted in the figure, when performance scores are reported with 4 decimal places, which is typical in the field, the probability of detecting inconsistencies due to the improper use of minority oversampling is 0.71. While there remains a 29\% chance of false negatives, the outcome suggests that the proposed technique effectively identifies this specific type of methodological flaw. As anticipated, the power declines when the scores are reported with fewer digits. It is worth noting that similar power analyses could be conducted for various other types of inconsistencies, such as assuming typographical errors in the reported scores.


\begin{figure}
    \begin{center}
        \includegraphics[width=0.35\textwidth]{power-tpehg.eps}
    \end{center}
    \caption{{The results of the power analysis for the TPEHG preterm delivery dataset. The assumed inconsistency is that minority oversampling is carried out prior to 5-fold cross-validation. The vertical axis represents the power of the test ($1 - \beta$), as a function of the number of decimal places reported.
    }}
    \label{fig:powertpehg}
\end{figure}

Beyond eliminating the need to reimplement algorithms to identify methodological flaws in this specific application, the example also highlights that the proposed consistency tests are effective for detecting similar methodological flaws in various other fields where minority oversampling \cite{smote} is used.

\subsection{Classification of skin lesions}
\label{sec:third}

In this subsection, we illustrate the application of the proposed techniques in a field where -- to the best of our knowledge -- no meta-analysis aimed at validating  reported results has been conducted before: the classification of skin lesion images. An analysis as detailed as the one we conducted in retinal vessel segmentation \cite{vessel} is clearly beyond the scope of this paper. However, we can test the reported scores in some highly influential papers to see if ambiguities are present. The analysis is based on the highly cited survey \cite{skinsurvey} providing a systematic overview of research up until 2021. From this survey, we selected the 10 papers (listed in Table \ref{tab:skin}) with the most citations according to Google Scholar at the time of writing.

Unlike the applications discussed in Subsections \ref{sec:retina} and \ref{sec:ehg}, this field is centered around multiple datasets (see Table \ref{tab:skin} for a summary) compiled for the classification of skin lesion images into two classes (malignant and non-malignant, ISIC2016 \cite{isic2016}) or multiple, more specific categories (ISIC2017 \cite{isic2017}, see Figure \ref{figskin} for an illustration). After carefully analyzing the selected papers, we found that \cite{skin1}, \cite{skin4}, and \cite{skin6} are not suitable for consistency testing (refer to the 'conclusion' column of Table \ref{tab:skin} for details). In the remaining papers, the authors provide sufficient details to apply the consistency tests. (Note that the 3-class problem ISIC2017 \cite{isic2017} was treated by the authors as two one-vs-all binary classification problems targeting the recognition of the classes \emph{Melanoma} (M) and \emph{Seborrheic Keratosis} (SK)). 
In most papers, multiple sets of performance scores are shared, illustrating the operation of certain algorithmic steps, with the most commonly reported scores being accuracy, sensitivity, and specificity. Given that the datasets either include a designated test set of images or the authors specify one and share its details, the consistency test developed in Section \ref{sec:ind} was applied in each case. The number of reported score sets ($N_{sc.}$) and the number of inconsistent sets ($N_{inc.}$) are provided in the corresponding columns of Table \ref{tab:skin}. No inconsistencies were detected in \cite{skin5} and \cite{skin8}. In \cite{skin0}, \cite{skin2}, and \cite{skin3}, only a handful of score sets showed inconsistencies, likely due to typographical errors. However, the scores reported in papers \cite{skin7} and \cite{skin9} were inconsistent with the assumptions of the experiment, prompting a more detailed analysis of these papers.

Regarding \cite{skin7}, we tested multiple assumptions, such as the possibility that scores like $M_{ACC}$ represent the accuracy of the \emph{Melanoma} (M) class against the \emph{Nevus} class instead of against both the \emph{Nevus} and \emph{Seborrheic Keratosis} classes. We also assumed that the accuracy and specificity figures might have been interchanged in the paper, as accuracy cannot be higher than both sensitivity and specificity simultaneously. However, all assumptions led to inconsistencies with the claimed number and distribution of test images. Therefore, we concluded that the reported scores are not comparable with those reported in other papers using the same dataset (such as \cite{skin3}). Regarding \cite{skin9}, the authors mention that they report the performance scores of binary classification in a weighted manner, from the perspective of both classes treated as positive and using the number of samples in a certain positive class as weights. This uncommon weighting makes sensitivity equivalent to accuracy and turns specificity into a figure with no common interpretation. Although this reinterpretation resolves the inconsistencies, we still consider the scores inconsistent with the commonly accepted definitions of the terms.

The final conclusion of the analysis is that, in the field of skin lesion classification, there are highly influential papers (such as \cite{skin7} and \cite{skin9}) with inconsistent scores that are often cited for comparison and ranking in other research (e.g., \cite{skin7ref}).

Similar to Subsection \ref{sec:ehg}, we carried out a power analysis to estimate how effective the proposed test is in recognizing inconsistencies in the skin lesion classification problems ISIC2016 and the two binary classification aspects of ISIC2017: recognizing the Melanoma class against Nevus and Seborrheic Keratosis, and recognizing Seborrheic Keratosis against Nevus and Melanoma. In this analysis, we are not aware of potential systematic methodological flaws (such as the improper use of minority oversampling in the case of preterm delivery prediction in Subsection \ref{sec:ehg}). Therefore, we investigated the ability of the test to recognize even the slightest typographical error in one of the reported scores, specifically when the last digit of accuracy is altered by 1. For example, instead of the true accuracy 0.932, accidentally 0.933 is reported. The power analysis was carried out in a similar manner as described in Subsection \ref{sec:ehg}, and the results are summarized in Figure \ref{fig:powerskin}. As the results suggest, when at least 3 decimal places are reported, the proposed test will recognize even the slightest typographical error as an inconsistency with a probability of 1, indicating its effectiveness in testing the consistency of reported scores in this field. It is worth highlighting that the scope of the power analysis does not limit the ability of the test to identify various other types of inconsistencies, such as adjustments to the evaluation set by changing the number of positive or negative samples (e.g., accidentally removing samples), or when performance scores are calculated using incorrect formulas. The power analyses for these types of inconsistencies can be carried out in a similar manner.


\begin{figure}[t]
\begin{center}
\subfloat[(a)][\emph{Nevus}\label{skinn}]{
\includegraphics[width=0.31\textwidth]{ISIC_0012147.jpg}
}
\subfloat[(b)][\emph{Seborrheic Keratosis}\label{skinsk}]{
\includegraphics[width=0.31\textwidth]{ISIC_0012134_sk.jpg}
}
\subfloat[(c)][\emph{Melanoma}\label{skinm}]{
\includegraphics[width=0.31\textwidth]{ISIC_0012369_m.jpg}
}
\end{center}
\caption{Entries of the ISIC 2017 \cite{isic2017} dataset: \emph{Nevus} (\ref{skinn}); \emph{Seborrheic Keratosis} (\ref{skinsk}); \emph{Melanoma} (\ref{skinm}).}
\label{figskin}
\end{figure}

\begin{table*}
\caption{The summary of consistency testing in skin lesion classification}
\label{tab:skin}
\begin{center}
\begin{footnotesize}
\begin{tabular}{l@{\hspace{5pt}}r@{\hspace{5pt}}p{75pt}@{\hspace{5pt}}r@{\hspace{5pt}}p{75pt}@{\hspace{5pt}}l@{\hspace{5pt}}l@{\hspace{5pt}}p{200pt}@{\hspace{5pt}}}
\toprule
ref. & cit. & dataset & digits & scores & $N_{sc.}$ & $N_{inc.}$ & conclusion \\
\midrule
\cite{skin0} & 991 & ISIC2016 \cite{isic2016} & 3 & acc, sens, spec & 18 & 1 & Potentially typos present. \\
\cite{skin1} & 603 & ISIC2016 \cite{isic2016} & - & - & - & - & The paper is about image segmentation performance. \\
\cite{skin2} & 574 & ISIC2016 \cite{isic2016} & 3 & acc, sens, spec & 27 & 3 & Potentially typos present. \\
\cite{skin3} & 389 & ISIC2017 \cite{isic2017} m/sk & 3 & acc, sens, spec & 32 & 2 & Potentially typos present. \\
\cite{skin4} & 389 & ISIC2016 \cite{isic2016} (custom selection) & - & - & - & - & Not enough details shared. \\
\cite{skin5} & 322 & Argenziano's \cite{argenziano} & 3 & ppv, sens, spec & 16 & 0 & No inconsistency identified. \\
\cite{skin6} & 313 & custom & - & - & - & - & Not enough details shared. \\
\cite{skin7} & 312 & ISIC2017 \cite{isic2017} m/sk & 3 & acc, sens, spec & 20 & 20 & All accuracy, sensitivity and specificity scores reported for the two binary classification tasks M and SK are inconsistent. \\
\cite{skin8} & 259 & custom & 4 & acc, sens, spec & 18 & 0 & No inconsistency identified. \\
\cite{skin9} & 238 & ISIC2016 \cite{isic2016} & 4 & acc, sens, spec, f1 & 8 & 4 & Unorthodox weighting of the scores. \\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\end{table*}

\begin{figure}
    \begin{center}
        \subfloat[][ISIC2016]{\includegraphics[width=0.31\textwidth]{power-isic2016.eps}}
        \subfloat[][ISIC2017 Melanoma]{\includegraphics[width=0.31\textwidth]{power-isic2017m.eps}}
        \subfloat[][ISIC2017 Seborrheic Keratosis]{\includegraphics[width=0.31\textwidth]{power-isic2017sk.eps}}
    \end{center}
    \caption{{The results of the power analysis for the ISIC2016 dataset (a) and the two binary aspects of the ISIC2017 dataset, recognizing Melanoma (b) and Seborrheic Keratosis (c). The assumed deviation is a typographical error with the smallest possible effect: a $10^{-k}$ difference in the $k$th digit of the accuracy score, when the results are reported to $k$ decimal places. The vertical axis represents the power of the test ($1 - \beta$) as a function of the number of decimal places reported. In these problems, when at least 3 digits are reported, the test recognizes the slightest deviation with a probability of 1. The slight decrease in power in the case of ISIC2016 at 4 decimal places is due to the fact that the size of the effect decreases with the increasing number of reported digits.}}
    \label{fig:powerskin}
\end{figure}

%%% Conclusion, further work %%%
\section{Conclusions}
\label{sec:conclusions}

The meta-analysis of research is crucial to address the reproducibility crisis in artificial intelligence research and applications. Nevertheless, without numerical techniques, it demands enormous manual labor. To facilitate meta-analysis and enable the numerical identification of methodological flaws and inconsistencies, we have introduced various numerical tests to assess the consistency of reported performance scores and experimental setups in binary classification. The tests are based on the fact that whenever multiple performance scores are reported, their values are interrelated, and these interrelations can be verified by numerical techniques.

The proposed tests cover numerous evaluation scenarios. The test developed for performance scores derived from a single evaluation set supports 20 different scores (Section \ref{sec:ind}). Regarding scores obtained by aggregations, we showed that in the case of the \emph{Score of Means} strategy, the testing falls back to the methods developed for scores derived from a single evaluation set (Subsection \ref{sec:rom}). For \emph{Mean of Scores} aggregations, we developed a test supporting four of the most commonly reported scores (Subsection \ref{sec:mor}), and we also proposed the enumeration of all fold configurations when stratification is not used and the specifics of the folds are unknown (Subsection \ref{sec:kfold}). Across Sections \ref{sec:ind} and \ref{sec:agg}, we highlighted multiple opportunities to improve the efficiency or extend the coverage of the tests.
Regarding the computational limitations, the test proposed for scores derived from a single confusion matrix is applicable to any reasonably sized dataset, while the tests developed for aggregated scores are limited by the capabilities of the integer linear programming solver being used.

In terms of applications, we briefly discussed the prior application of simplified forms of the proposed methods in the field of retinal vessel segmentation (Subsection \ref{sec:retina}) and explored potential further applications related to retinal image processing. 
We also demonstrated that the proposed techniques are suitable for replacing the manual labor required to verify the reported results by reimplementation in situations similar to preterm delivery prediction from EHG signals (Subsection \ref{sec:ehg}).
This application also illustrated that the proposed methods can be used to identify a common methodological pitfall when synthetic minority oversampling is employed. 
The power analysis of the method in this application reveals that the proposed test can recognize inconsistencies with a probability of 71\%.
Lastly, in Subsection \ref{sec:third}, through a small meta-analysis employing the proposed techniques in the field of skin lesion classification, we uncovered the presence of irreproducible results systematically cited in the literature. The power analysis focusing on typographical errors showed that, in this problem, the proposed method is capable of recognizing the slightest typographical errors with 100\% probability.

Given the reproducibility crisis in machine learning and artificial intelligence research, along with the vast number of scientific papers and the limited capacity for reimplementation, validation, and verification, the proposed tests -- as illustrated by the applications -- offer effective tools to assess and safeguard the integrity of research.

For the benefit of the community, a reference implementation of the proposed consistency tests has been released as an open-source Python package with an intuitive interface. The package is available in the standard Python repository (PyPI) under the name \verb|mlscorecheck| \cite{mlscorecheck} and on GitHub at the following URL: \url{https://github.com/FalseNegativeLab/mlscorecheck}.

%% Loading bibliography style file
% \bibliographystyle{model1-num-names}
\bibliographystyle{elsarticle-num}

% Loading bibliography database
\bibliography{references}

\end{document}
